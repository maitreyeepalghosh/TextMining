{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first let's do all of the import statements\n",
    "import requests\n",
    "import urllib2\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from string import punctuation\n",
    "from heapq import nlargest\n",
    "from math import log\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getWashPostText(url,token):\n",
    "    # THis function takes the URL of an article in the \n",
    "    # Washington Post, and then returns the article minus all \n",
    "    # of the crud - HTML, javascript etc. How? By searching for\n",
    "    # everything that lies between the tags titled 'token'\n",
    "    # Like most web-scraping, this will only work for urls where\n",
    "    # we know the structure (eg say all articles in the WashPo are\n",
    "    # enclosed in <article></article> tags). This will also change from\n",
    "    # time to time as different HTML formats are employed in the website\n",
    "    try:\n",
    "       # page = urllib2.urlopen(url).read().decode('utf8')\n",
    "        page = requests.get(url,verify=False)\n",
    "    except:\n",
    "        # if unable to download the URL, return title = None, article = None\n",
    "        return (None,None)\n",
    "    soup = BeautifulSoup(page)\n",
    "    if soup is None:\n",
    "        return (None,None)\n",
    "    # If we are here, it means the error checks were successful, we were\n",
    "    # able to parse the page\n",
    "    text = \"\"\n",
    "    if soup.find_all(token) is not None:\n",
    "        # Search the page for whatever token demarcates the article\n",
    "        # usually '<article></article>'\n",
    "        text = ''.join(map(lambda p: p.text, soup.find_all(token)))\n",
    "        # mush together all the text in the <article></article> tags\n",
    "        soup2 = BeautifulSoup(text)\n",
    "        # create a soup of the text within the <article> tags\n",
    "        if soup2.find_all('p')!=[]:\n",
    "            # now mush together the contents of what is in <p> </p> tags\n",
    "            # within the <article>\n",
    "            text = ''.join(map(lambda p: p.text, soup2.find_all('p')))\n",
    "    return text, soup.title.text\n",
    "    # what did we just do? Let's go through and understand\n",
    "    # finally return the result tuple with the title and the body of the article\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Now we will do something very very similar, but this time for the New York Times\n",
    "def getNYTText(url,token):\n",
    "    response = requests.get(url)\n",
    "    # THis is an alternative way to get the contents of a URL\n",
    "    soup = BeautifulSoup(response.content,\"lxml\")\n",
    "    page = str(soup)\n",
    "    title = soup.find('title').text\n",
    "    mydivs = soup.findAll(\"p\", {\"class\":\"story-body-text story-content\"})\n",
    "    text = ''.join(map(lambda p:p.text, mydivs))\n",
    "    return text, title\n",
    "    # Notice again how important it is to know the structure of the page\n",
    "    # we are seeking to scrape. If we did not know that articles in the NYT\n",
    "    # come contained in these tags - an outer tag <p> and an inner tag\n",
    "    # of class = story-body-text story-content, we would be unable to parse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Ok! Now we have a way to extract the contents and title of an individual\n",
    "# URL. Let's hook this up inside another function that will take the URL\n",
    "# of an entire section of a newspaper - say the Technology or Sports section\n",
    "# of a newspaper - and parse all of the URLs for articles linked off that\n",
    "# section. \n",
    "# Btw, these sections also come with plenty of non-news links - 'about',\n",
    "# how to syndicate etc, so we will employ a little hack - we will consider\n",
    "# something to be a news article only if the url has a dateline. THis is \n",
    "# actually very safe - its pretty much the rule for articles to have a \n",
    "# date, and virtually all important newspapers mush this date into the URL.\n",
    "def scrapeSource(url, magicFrag='2015',scraperFunction=getNYTText,token='None'):\n",
    "    urlBodies = {}\n",
    "    request = urllib2.Request(url)\n",
    "    response = urllib2.urlopen(request)\n",
    "    soup = BeautifulSoup(response)\n",
    "    # we are set up with a Soup of the page - now find the links\n",
    "    # Remember that links are always of the form \n",
    "    # <a href='link-url'> link-text </a>\n",
    "    numErrors = 0\n",
    "    for a in soup.findAll('a'):\n",
    "        try:\n",
    "            url = a['href']\n",
    "            if( (url not in urlBodies) and \n",
    "               ((magicFrag is not None and magicFrag in url) \n",
    "               or magicFrag is None)):\n",
    "                body = scraperFunction(url,token)\n",
    "                # this line above is important - scraperFunction \n",
    "                # refers to the individual scraper function for the \n",
    "                # new york times or the washington post etc.\n",
    "                if body and len(body) > 0:\n",
    "                    urlBodies[url] = body\n",
    "                print url\n",
    "        except:\n",
    "            numErrors += 1\n",
    "            # plenty of parse errors happen - links might not\n",
    "            # be external links, might be malformed and so on -\n",
    "            # so don't mind if there are exceptions.\n",
    "    return urlBodies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Now for the frequency summarizer class - which we have encountered\n",
    "# before. To quickly jog our memories - given an (title,article-body) tuple\n",
    "# the frequency summarizer has easy ways to find the most 'important'\n",
    "# sentences, and the most important words. How is 'important' defined?\n",
    "# Important = most frequent, excluding 'stopwords' which are generic\n",
    "# words like 'the' etc which can be ignored\n",
    "class FrequencySummarizer:\n",
    "    def __init__(self,min_cut=0.1,max_cut=0.9):\n",
    "        # class constructor - takes in min and max cutoffs for \n",
    "        # frequency\n",
    "        self._min_cut = min_cut\n",
    "        self._max_cut = max_cut\n",
    "        self._stopwords = set(stopwords.words('english') +\n",
    "                              list(punctuation) +\n",
    "                              [u\"'s\",'\"'])\n",
    "        # notice how the stopwords are a set, not a list. \n",
    "        # its easy to go from set to list and vice-versa\n",
    "        # (simply use the set() and list() functions) - \n",
    "        # but conceptually sets are different from lists\n",
    "        # because sets don't have an order to their elements\n",
    "        # while lists do\n",
    "    \n",
    "    def _compute_frequencies(self,word_sent,customStopWords=None):\n",
    "        freq = defaultdict(int)\n",
    "        # we have encountered defaultdict objects before\n",
    "        if customStopWords is None:\n",
    "            stopwords = set(self._stopwords)\n",
    "        else:\n",
    "            stopwords = set(customStopWords).union(self._stopwords)\n",
    "        for sentence in word_sent:\n",
    "            for word in sentence:\n",
    "                if word not in stopwords:\n",
    "                    freq[word] += 1\n",
    "        m = float(max(freq.values()))\n",
    "        for word in freq.keys():\n",
    "            freq[word] = freq[word]/m\n",
    "            if freq[word] >= self._max_cut or freq[word] <= self._min_cut:\n",
    "                del freq[word]\n",
    "        return freq\n",
    "    \n",
    "    def extractFeatures(self,article,n,customStopWords=None):\n",
    "        # The article is passed in as a tuple (text, title)\n",
    "        text = article[0]\n",
    "        # extract the text\n",
    "        title = article[1]\n",
    "        # extract the title\n",
    "        sentences = sent_tokenize(text)\n",
    "        # split the text into sentences\n",
    "        word_sent = [word_tokenize(s.lower()) for s in sentences]\n",
    "        # split the sentences into words \n",
    "        self._freq = self._compute_frequencies(word_sent,customStopWords)\n",
    "        # calculate the word frequencies using the member function above\n",
    "        if n < 0:\n",
    "            # how many features (words) to return? IF the user has\n",
    "            # asked for a negative number, this is a sign that we don't\n",
    "            # do any feature selection - we return ALL features\n",
    "            # THis is feature extraction without any pruning, ie no\n",
    "            # feature selection (beyond simply picking words as the features)\n",
    "            return nlargest(len(self._freq_keys()),self._freq,key=self._freq.get)\n",
    "        else:\n",
    "            # if the calling function has asked for a subset then\n",
    "            # return only the 'n' largest features - ie here the most\n",
    "            # important words (important == frequent, barring stopwords)\n",
    "            return nlargest(n,self._freq,key=self._freq.get)\n",
    "        # let's summarize what we did here. \n",
    "    \n",
    "    def extractRawFrequencies(self, article):\n",
    "        # very similar, except that this method will return the 'raw'\n",
    "        # frequencies - literally just the word counts\n",
    "        text = article[0]\n",
    "        title = article[1]\n",
    "        sentences = sent_tokenize(text)\n",
    "        word_sent = [word_tokenize(s.lower()) for s in sentences]\n",
    "        freq = defaultdict(int)\n",
    "        for s in word_sent:\n",
    "            for word in s:\n",
    "                if word not in self._stopwords:\n",
    "                    freq[word] += 1\n",
    "        return freq\n",
    "    \n",
    "    def summarize(self, article,n):\n",
    "        text = article[0]\n",
    "        title = article[1]\n",
    "        sentences = sent_tokenize(text)\n",
    "        word_sent = [word_tokenize(s.lower()) for s in sentences]\n",
    "        self._freq = self._compute_frequencies(word_sent)\n",
    "        ranking = defaultdict(int)\n",
    "        for i,sentence in enumerate(word_sent):\n",
    "            for word in sentence:\n",
    "                if word in self._freq:\n",
    "                    ranking[i] += self._freq[word]\n",
    "        sentences_index = nlargest(n,ranking,key=ranking.get)\n",
    "\n",
    "        return [sentences[j] for j in sentences_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 403: Forbidden",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-fe5469933d83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m                                           \u001b[1;34m'2016'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                                          \u001b[0mgetWashPostText\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                                          'article') \n\u001b[0m\u001b[1;32m     11\u001b[0m washingtonPostNonTechArticles = scrapeSource(urlWashingtonPostNonTech,\n\u001b[1;32m     12\u001b[0m                                           \u001b[1;34m'2016'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-32aedf479581>\u001b[0m in \u001b[0;36mscrapeSource\u001b[0;34m(url, magicFrag, scraperFunction, token)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0murlBodies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mrequest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murllib2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murllib2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[1;31m# we are set up with a Soup of the page - now find the links\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Saurabh\\Anaconda2\\lib\\urllib2.pyc\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0mopener\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Saurabh\\Anaconda2\\lib\\urllib2.pyc\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Saurabh\\Anaconda2\\lib\\urllib2.pyc\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m             response = self.parent.error(\n\u001b[0;32m--> 548\u001b[0;31m                 'http', request, response, code, msg, hdrs)\n\u001b[0m\u001b[1;32m    549\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Saurabh\\Anaconda2\\lib\\urllib2.pyc\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'default'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'http_error_default'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[1;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Saurabh\\Anaconda2\\lib\\urllib2.pyc\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Saurabh\\Anaconda2\\lib\\urllib2.pyc\u001b[0m in \u001b[0;36mhttp_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m         \u001b[1;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_full_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 403: Forbidden"
     ]
    }
   ],
   "source": [
    "\n",
    "urlWashingtonPostNonTech = \"https://www.washingtonpost.com/sports\"\n",
    "urlNewYorkTimesNonTech = \"https://www.nytimes.com/pages/sports/index.html\"\n",
    "urlWashingtonPostTech = \"https://www.washingtonpost.com/business/technology\"\n",
    "urlNewYorkTimesTech = \"http://www.nytimes.com/pages/technology/index.html\"\n",
    "\n",
    "washingtonPostTechArticles = scrapeSource(urlWashingtonPostTech,\n",
    "                                          '2016',\n",
    "                                         getWashPostText,\n",
    "                                         'article') \n",
    "washingtonPostNonTechArticles = scrapeSource(urlWashingtonPostNonTech,\n",
    "                                          '2016',\n",
    "                                         getWashPostText,\n",
    "                                         'article')\n",
    "                \n",
    "                \n",
    "newYorkTimesTechArticles = scrapeSource(urlNewYorkTimesTech,\n",
    "                                       '2016',\n",
    "                                       getNYTText,\n",
    "                                       None)\n",
    "newYorkTimesNonTechArticles = scrapeSource(urlNewYorkTimesNonTech,\n",
    "                                       '2016',\n",
    "                                       getNYTText,\n",
    "                                       None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
