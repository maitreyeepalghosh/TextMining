

https://github.com/cegme/gatordsr/wiki/Tutorial-of-Stanford-CoreNLP


static StanfordCoreNLP	tokenPipeline;

	static {
		Properties props = new Properties();
		props.setProperty("annotators", "tokenize, ssplit");
		tokenPipeline = new StanfordCoreNLP(props);
	}
	
	public List<String> doTokenization(final String content) {
		List<String> token_list = new ArrayList<String>();

		Annotation document = new Annotation(content);
		tokenPipeline.annotate(document);
		List<CoreMap> sentences = document.get(SentencesAnnotation.class);

		for (CoreMap sentence : sentences) {
			for (CoreLabel token : sentence.get(TokensAnnotation.class)) {
				token_list.add(token.originalText().toString());
			}
		}
		System.out.println(token_list);
		return token_list;
	}
  -------------------------------------------------------------------
  	static StanfordCoreNLP	sspliPipeline;
	
	

	static {

		Properties props = new Properties();
		props.setProperty("annotators", "tokenize, ssplit");
		sspliPipeline = new StanfordCoreNLP(props);

	}

	public List<String> splitTextIntoSentences(final String content) {

		List<String> sentences_list = new LinkedList<String>();
		Annotation document = new Annotation(content);
		sspliPipeline.annotate(document);
		List<CoreMap> sentences = document.get(SentencesAnnotation.class);
		for (CoreMap sentence : sentences) {
			sentences_list.add(sentence.toString());
			System.out.println(sentence.toString());

		}

		return sentences_list;
	}
  
  --------------------------------------------------------------------------------
  
  https://github.com/smilli/CoreNLP/blob/master/itest/src/edu/stanford/nlp/ling/tokensregex/TokenSequenceMatcherITest.java
  
  public void testTokenSequenceMatcherBeginEnd() throws IOException {
    CoreMap doc = createDocument(testText);

    // Test simple sequence with begin sequence matching
    TokenSequencePattern p = TokenSequencePattern.compile("^ [] []");
    TokenSequenceMatcher m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));

    boolean match = m.find();
    assertTrue(match);
    assertEquals("the number", m.group());

    match = m.find();
    assertFalse(match);

    // Test simple sequence with end sequence matching
    p = TokenSequencePattern.compile("[] [] $");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));

    match = m.find();
    assertTrue(match);
    assertEquals("fifty.", m.group());

    match = m.find();
    assertFalse(match);

    // Test simple sequence with begin and end sequence matching
    p = TokenSequencePattern.compile("^ [] [] $");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));

    match = m.find();
    assertFalse(match);

    // Test simple sequence with ^$ in a string regular expression
    p = TokenSequencePattern.compile("/^number$/");
    m = p.getMatcher(doc.get(CoreAnnotations.TokensAnnotation.class));

    match = m.find();
    assertTrue(match);
    assertEquals("number", m.group());
  
  
----------------------------------------------------------------------------------------------------

https://www.npmjs.com/package/stanford-corenlp


What is Stanford CoreNLP?
Stanford CoreNLP provides a set of natural language analysis tools which can take raw English language text input and give the base forms of words, their parts of speech, whether they are names of companies, people, etc., normalize dates, times, and numeric quantities, and mark up the structure of sentences in terms of phrases and word dependencies, and indicate which noun phrases refer to the same entities. Stanford CoreNLP is an integrated framework, which make it very easy to apply a bunch of language analysis tools to a piece of text. Starting from plain text, you can run all the tools on it with just two lines of code. Its analyses provide the foundational building blocks for higher-level and domain-specific text understanding applications.

Stanford CoreNLP integrates all our NLP tools, including the part-of-speech (POS) tagger, the named entity recognizer (NER), the parser, and the coreference resolution system, and provides model files for analysis of English. The goal of this project is to enable people to quickly and painlessly get complete linguistic annotations of natural language texts. It is designed to be highly flexible and extensible. With a single option you can change which tools should be enabled and which should be disabled.

The Stanford CoreNLP code is written in Java and licensed under the GNU General Public License (v2 or later). Source is included. Note that this is the full GPL, which allows many free uses, but not its use in distributed proprietary software. The download is 214 MB and requires Java 1.6+



public List<String> getPhrases(String content,String type) {

		//String type="NP";
		Standford_ParseTree obj = new Standford_ParseTree();
		List<Tree> parseTree = obj.createParseTree(content);
		ArrayList<String> allNP = new ArrayList<String>();
		for (Tree treeAnnot : parseTree) {

			allNP.addAll(getChunk(treeAnnot,type));

		}
		System.out.println(allNP);
		return allNP;
	}

	private static ArrayList<String> getChunk(Tree treeAnnot,String type) {

		ArrayList<String> nounPhrase = new ArrayList<String>();
		TregexPattern tgrepPattern = TregexPattern.compile(type);
		TregexMatcher m = tgrepPattern.matcher(treeAnnot);
		while (m.find()) {
			List<Tree> list = new ArrayList<Tree>();
			String phrase = "";

			Tree matchedTree = m.getMatch();

			if (matchedTree.toString().startsWith("("+type)
					&& matchedTree.toString().matches(".*\\w{1,}.*\\("+type+".*")) {
				continue;
			}

			list = iterateTree(matchedTree, list);
			for (int i = 0; i < list.size(); i++) {
				String extractedPhrase = "";
				extractedPhrase = list.get(i).toString();
				phrase = phrase
						+ " "
						+ extractedPhrase.replace(list.get(i).label()
								.toString(), "");
				
			}
			if(!phrase.trim().matches("\\d{0,}")){
					phrase.replace("\\d{0,}", "");
					if(!phrase.isEmpty())
					nounPhrase.add(phrase);
				}
		}
		
		return nounPhrase;
	}

	private static List<Tree> iterateTree(Tree t, List<Tree> list) {
		List<Tree> subTree = t.getChildrenAsList();

		for (Tree tree : subTree) {

			if (tree.isPhrasal() || !tree.isLeaf()) {

				iterateTree(tree, list);
			} else {
				list.add(tree);
			}

		}
		return list;

	}
  -----------------------------------------------
  
  
  
  List<Tree> tree = new ArrayList<>();

		Annotation document = new Annotation(content);
		parsePipeline.annotate(document);
		List<CoreMap> sentences = document.get(SentencesAnnotation.class);
		for (CoreMap sentence : sentences) {
			tree.add(sentence.get(TreeAnnotation.class));

		}
	
  ---------------------------------------------------------------------------------------------
  
  static StanfordCoreNLP	posTaggerPipeline;

	static {

		Properties props = new Properties();
		props.setProperty("annotators", "tokenize, ssplit, pos");
		posTaggerPipeline = new StanfordCoreNLP(props);

	}


	public List<String>  doPosTagging(final String content) {
		
	List<String> result=new ArrayList<String>();
		Annotation document = new Annotation(content);
		posTaggerPipeline.annotate(document);
		List<CoreMap> sentences = document.get(SentencesAnnotation.class);
		for (CoreMap sentence : sentences) {
			// Iterate over all tokens in a sentence
			for (CoreLabel token : sentence.get(TokensAnnotation.class)) {

				String word = token.originalText()
						+"||"
						+ token.tag();
				//posTaggedWords = posTaggedWords + "  " + word;
				result.add(word);
			}
		}

		return result;

	}

	public HashSet<String> getAdjective(final String content) {

		HashSet<String> adjSet = new HashSet<>();
		Annotation document = new Annotation(content);
		posTaggerPipeline.annotate(document);
		List<CoreMap> sentences = document.get(SentencesAnnotation.class);
		for (CoreMap sentence : sentences) {
			// Iterate over all tokens in a sentence
			for (CoreLabel token : sentence.get(TokensAnnotation.class)) {

				if (token.tag().equalsIgnoreCase("JJ")) {

					adjSet.add(token.originalText());
				}
			}
		}

		return adjSet;
	}
----------------------------------------------------------------------------------------------------


for (CoreMap sentence : sentences)
{
    for (CoreLabel token : sentence.get(TokensAnnotation.class))
    {
        // Using the CoreLabel object we can start retrieving NLP annotation data
        // Extracting the Text Entity
        String text = token.getString(TextAnnotation.class);

        // Extracting Name Entity Recognition 
        String ner = token.getString(NamedEntityTagAnnotation.class);

        // Extracting Part Of Speech
        String pos = token.get(CoreAnnotations.PartOfSpeechAnnotation.class);

        // Extracting the Lemma
        String lemma = token.get(CoreAnnotations.LemmaAnnotation.class);
        System.out.println("text=" + text + ";NER=" + ner +
                        ";POS=" + pos + ";LEMMA=" + lemma);



---------------------------------------------------------------------------------------

// creates a StanfordCoreNLP object
// with POS tagging, lemmatization, NER, parsing, and coreference resolution
Properties props = new Properties();
props.put("annotators", "tokenize, ssplit, pos, lemma, ner, parse, dcoref");
StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

// read some text in the text variable
String text = ... // Add your text here!

// create an empty Annotation just with the given text
Annotation document = new Annotation(text);

// run all Annotators on this text
pipeline.annotate(document);`
// these are all the sentences in this document
// a CoreMap is essentially a Map that uses class objects as keys and has values with custom types
List<CoreMap> sentences = document.get(SentencesAnnotation.class);

for(CoreMap sentence: sentences) {
  // traversing the words in the current sentence
  // a CoreLabel is a CoreMap with additional token-specific methods
  for (CoreLabel token: sentence.get(TokensAnnotation.class)) {
    // this is the text of the token
    String word = token.get(TextAnnotation.class);
    // this is the POS tag of the token
    String pos = token.get(PartOfSpeechAnnotation.class);
    // this is the NER label of the token
    String ne = token.get(NamedEntityTagAnnotation.class);       
  }

  // this is the parse tree of the current sentence
  Tree tree = sentence.get(TreeAnnotation.class);

  // this is the Stanford dependency graph of the current sentence
  SemanticGraph dependencies = sentence.get(CollapsedCCProcessedDependenciesAnnotation.class);
}

// This is the coreference link graph
// Each chain stores a set of mentions that link to each other,
// along with a method for getting the most representative mention
// Both sentence and token offsets start at 1!
Map<Integer, CorefChain> graph = 
  document.get(CorefChainAnnotation.class);
  
  
  
  
  ----------------------------------------------------------------------------------
  corenlp website
  
  import java.io.BufferedReader;
import java.io.File;
import java.io.FileInputStream;
import java.io.IOException;
import java.io.InputStreamReader;
import java.util.ArrayList;
import java.util.List;
import java.util.Map;

import edu.stanford.nlp.ling.CoreLabel;
import junit.framework.TestCase;


/** @author John Bauer */
public class PTBTokenizerITest extends TestCase {

  private static void compareResults(BufferedReader testReader,
                             List<String> goldResults) {
    PTBTokenizer<CoreLabel> tokenizer =
            new PTBTokenizer<>(testReader, new CoreLabelTokenFactory(), "");
    List<String> testResults = new ArrayList<>();
    while (tokenizer.hasNext()) {
      CoreLabel w = tokenizer.next();
      testResults.add(w.word());
    }

    // Compare tokens before checking size so get better output if unequal
    int compareSize = Math.min(goldResults.size(), testResults.size());
    for (int i = 0; i < compareSize; ++i) {
      assertEquals(goldResults.get(i), testResults.get(i));
    }
    assertEquals(goldResults.size(), testResults.size());
  }

  private static BufferedReader getReaderFromInJavaNlp(String filename)
      throws IOException {
    final String charset = "utf-8";
    BufferedReader reader;
    try {
      reader = new BufferedReader
      (new InputStreamReader
       (PTBTokenizerITest.class.getResourceAsStream(filename), charset));
    } catch (NullPointerException npe) {
      Map<String,String> env = System.getenv();
      String path = "projects/core/data/edu/stanford/nlp/process" + File.separator + filename;
      String loc = env.get("JAVANLP_HOME");
      if (loc != null) {
        path = loc + File.separator + path;
      }
      reader = new BufferedReader(new InputStreamReader(new FileInputStream(path), charset));
    }
    return reader;
  }

  public void testLargeDataSet()
    throws IOException
  {
    BufferedReader goldReader = getReaderFromInJavaNlp("ptblexer.gold");
    List<String> goldResults = new ArrayList<String>();
    for (String line; (line = goldReader.readLine()) != null; ) {
      goldResults.add(line.trim());
    }

    BufferedReader testReader = getReaderFromInJavaNlp("ptblexer.test");
    compareResults(testReader, goldResults);

    testReader = getReaderFromInJavaNlp("ptblexer.crlf.test");
    compareResults(testReader, goldResults);
  }

}
Contact GitHub 
