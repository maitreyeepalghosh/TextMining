import json
import urllib
import urllib2
from unidecode import unidecode
import time

content="In machine learning and natural language processing, a topic model is a type of statistical model for discovering the abstract topics that occur in a collection of documents."
count = 0
start = time.time()
#while count < 1000:                
    urlPostPrefixSpotlight = "http://spotlight.sztaki.hu:2222/rest/annotate"
    args = urllib.urlencode([("text", content)])
    request = urllib2.Request(urlPostPrefixSpotlight, data=args, headers={"Accept": "application/json"})
    response = urllib2.urlopen(request).read()
    #print response
    #count += 1
end = time.time()
elapsed_seconds = float("%.2f" % (end - start))
print('%s: elapsed seconds: %s', elapsed_seconds)


pydict= json.loads(response)

json.dumps(pydict)

def parseJsontoList(pydict):     
			annotation =  pydict['Resources']
			entries = set()
			links=set()
			for keyword in annotation:
				#print("===>",keyword)
				if keyword["@URI"] not in links:
					entries.add(keyword["@surfaceForm"])
					links.add(keyword["@URI"])
			print("**********************",entries)                
			return entries
			
parseJsontoList(pydict)




#if 'Resources' in pydict:
#				annotation =  pydict['Resources']
				#return HttpResponse(annotation)
				#print response
				#print request.data
#				entries = {}
#				for keyword in annotation:
#						print keyword["@URI"]
#						if keyword["@URI"] not in entries.values():
#							if keyword["@types"] !="":
#								types=str(keyword["@types"]).split(',')                                
#								types=filter(lambda x: x.split(':')[0]!='Http',list(types))
#								types=map(lambda x:x.split(':')[1],list(types))
#								print types
#								entries[keyword["@surfaceForm"]] = ",".join(types)
#							else:
#								entries[keyword["@surfaceForm"]]=keyword["@types"]
#				print json.dumps(entries)

######################################################################################################################################



from __future__ import absolute_import
from __future__ import division, print_function, unicode_literals
import sys
from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer

#Summarizers
from sumy.summarizers.lsa import LsaSummarizer as Summarizer
from sumy.summarizers.luhn import LuhnSummarizer
from sumy.summarizers.edmundson import EdmundsonSummarizer
from sumy.summarizers.lsa import LsaSummarizer
from sumy.summarizers.text_rank import TextRankSummarizer
from sumy.summarizers.lex_rank import LexRankSummarizer
from sumy.summarizers.sum_basic import SumBasicSummarizer
from sumy.summarizers.kl import KLSummarizer
from sumy.nlp.stemmers import Stemmer
from sumy.utils import get_stop_words

#Evaluation
from sumy.evaluation.rouge import rouge_1
from sumy.evaluation.coselection import  f_score,precision,recall
from sumy.evaluation.content_based import  cosine_similarity
from sumy.models import TfDocumentModel
from itertools import chain



LANGUAGE = "english"
SENTENCES_COUNT = 5
#AVAILABLE_METHODS = [LuhnSummarizer,EdmundsonSummarizer,LsaSummarizer,TextRankSummarizer,LexRankSummarizer,SumBasicSummarizer,KLSummarizer]
                    
documentText = "In machine learning and natural language processing, a topic model is a type of statistical model for discovering the abstract topics that occur in a collection of documents. "

parser = PlaintextParser.from_string(documentText, Tokenizer(LANGUAGE))
print(parser.document.sentences)
stemmer = Stemmer(LANGUAGE)
summarizer_class = LuhnSummarizer


#####################################################################

#stemmer = Stemmer(LANGUAGE)
#summarizer = Summarizer(stemmer)
#summarizer.stop_words = get_stop_words(LANGUAGE)

#for sentence in summarizer(parser.document, SENTENCES_COUNT):
#    print(sentence)





def build_summarizer(summarizer_class, stop_words, stemmer, parser):
        summarizer = summarizer_class(stemmer)
        if summarizer_class is EdmundsonSummarizer:
            summarizer.null_words = stop_words
            summarizer.bonus_words = parser.significant_words
            summarizer.stigma_words = parser.stigma_words
        else:
            summarizer.stop_words = stop_words
        return summarizer
		
		
    
#Content based evalutaion
def evaluate_cosine_similarity(evaluated_sentences, reference_sentences):
    evaluated_words = tuple(chain(*(s.words for s in evaluated_sentences)))
    reference_words = tuple(chain(*(s.words for s in reference_sentences)))
    evaluated_model = TfDocumentModel(evaluated_words)
    reference_model = TfDocumentModel(reference_words)
    #print("cosine similarity:#####===>",cosine_similarity(evaluated_model, reference_model))
    return cosine_similarity(evaluated_model, reference_model)   



summarizer = build_summarizer(summarizer_class,get_stop_words(LANGUAGE),stemmer,parser)

#outputFile=open("result.txt","w+")
summary=summarizer(parser.document, SENTENCES_COUNT)

sentences=[str(sentence) for sentence in summary]
#print("==>",sentences)
#print("************************",rouge_1(summary,parser.document.sentences))
print(" ".join(sentences),"\n",evaluate_cosine_similarity(summary,parser.document.sentences))	


##################################################################################################################################

import json
import nltk
import numpy

BLOG_DATA = "D:\\Maitreyee\input\\test3\\Bangalore traffic report.txt"

N = 100  # Number of words to consider
CLUSTER_THRESHOLD = 5  # Distance between words to consider
TOP_SENTENCES = 2  # Number of sentences to return for a "top n" summary


# Approach taken from "The Automatic Creation of Literature Abstracts" by H.P. Luhn

def _score_sentences(sentences, important_words):
    scores = []
    sentence_idx = -1

    for s in [nltk.tokenize.word_tokenize(s) for s in sentences]:

        sentence_idx += 1
        word_idx = []

        # For each word in the word list...
        for w in important_words:
            try:
                # Compute an index for where any important words occur in the sentence.

                word_idx.append(s.index(w))
            except ValueError, e:  # w not in this particular sentence
                pass

        word_idx.sort()

        # It is possible that some sentences may not contain any important words at all.
        if len(word_idx) == 0: continue

        # Using the word index, compute clusters by using a max distance threshold
        # for any two consecutive words.

        clusters = []
        cluster = [word_idx[0]]
        i = 1
        while i < len(word_idx):
            if word_idx[i] - word_idx[i - 1] < CLUSTER_THRESHOLD:
                cluster.append(word_idx[i])
            else:
                clusters.append(cluster[:])
                cluster = [word_idx[i]]
            i += 1
        clusters.append(cluster)

        # Score each cluster. The max score for any given cluster is the score
        # for the sentence.

        max_cluster_score = 0
        for c in clusters:
            significant_words_in_cluster = len(c)
            total_words_in_cluster = c[-1] - c[0] + 1
            score = 1.0 * significant_words_in_cluster \
                    * significant_words_in_cluster / total_words_in_cluster

            if score > max_cluster_score:
                max_cluster_score = score

        scores.append((sentence_idx, score))

    return scores


def summarize(txt):
    sentences = [s for s in nltk.tokenize.sent_tokenize(txt)]
    normalized_sentences = [s.lower() for s in sentences]

    words = [w.lower() for sentence in normalized_sentences for w in
             nltk.tokenize.word_tokenize(sentence)]

    fdist = nltk.FreqDist(words)

    top_n_words = [w[0] for w in fdist.items()
                   if w[0] not in nltk.corpus.stopwords.words('english')][:N]

    scored_sentences = _score_sentences(normalized_sentences, top_n_words)

    # Summarization Approach 1:
    # Filter out nonsignificant sentences by using the average score plus a
    # fraction of the std dev as a filter

    avg = numpy.mean([s[1] for s in scored_sentences])
    std = numpy.std([s[1] for s in scored_sentences])
    mean_scored = [(sent_idx, score) for (sent_idx, score) in scored_sentences
                   if score > avg + 0.5 * std]

    # Summarization Approach 2:
    # Another approach would be to return only the top N ranked sentences

    top_n_scored = sorted(scored_sentences, key=lambda s: s[1])[-TOP_SENTENCES:]
    top_n_scored = sorted(top_n_scored, key=lambda s: s[0])

    # Decorate the post object with summaries

    return dict(top_n_summary=[sentences[idx] for (idx, score) in top_n_scored],
                mean_scored_summary=[sentences[idx] for (idx, score) in mean_scored])




text="The original paper is easy to understand and rather interesting. Luhn actually describes how he prepared punch cards in order to run various tests with different parameters. It is amazing to think that what we can implement in a few dozen lines of Python on a cheap piece of commodity hardware, he probably labored over for hours and hours to program into a gargantuan mainframe."
dict=summarize(text)
print dict

#blog_data = json.loads(open(BLOG_DATA).read())

#for post in blog_data:
 #   post.update(summarize(post['content']))

#    print post['title']
#   print '=' * len(post['title'])
#    print
#    print 'Top N Summary'
#  print '-------------'
#   print ' '.join(post['top_n_summary'])
#    print
#    print 'Mean Scored Summary'
#    print '-------------------'
#    print ' '.join(post['mean_scored_summary'])
#    print


##########################################################################################################



private static TokenizerME tokenizer = null;
	private static String OpenNlpPTokenModelPath = ParamConfiguration
			.getInstance().getOPENNLP_RESOURCE_TOKEN_MODEL();

	static {
		try {
			InputStream is = new FileInputStream(OpenNlpPTokenModelPath);

			TokenizerModel model = new TokenizerModel(is);

			tokenizer = new TokenizerME(model);
			System.out.println("tokens");

		} catch (Exception e) {
			e.printStackTrace();
		}

	}

	
	public List<String> doTokenization(final String content) {
		String tokens[] = tokenizer.tokenize(content);

		List<String> tokList = Arrays.asList(tokens);
		return tokList;
	}
######################################################################################


 static SentenceDetectorME sdetector = null;

	 static String openNlpSenModelPath = ParamConfiguration
			.getInstance().getOPENNLP_RESOURCE_SEN_MODEL();
	
	static {

		try {

			InputStream is =ParamConfiguration.getInstance().getInputStreamFromFile(openNlpSenModelPath);
			
			SentenceModel model = new SentenceModel(is);
			sdetector = new SentenceDetectorME(model);

		} catch (Exception e) {
			e.printStackTrace();
		}

	}

	
	public List<String> splitTextIntoSentences(final String content) {

		String sentences[] = sdetector.sentDetect(content);

		List<String> sentList = Arrays.asList(sentences);

		return sentList;
	}
###############################################################################################
private static POSTaggerME posTagger = null;
	static String OpenNlpPosModelPath = ParamConfiguration.getInstance()
			.getOPENNLP_RESOURCE_POS_MODEL();

	static {
		try {

			InputStream is =ParamConfiguration.getInstance().getInputStreamFromFile(OpenNlpPosModelPath);
			POSModel model = new POSModel(is);
			posTagger = new POSTaggerME(model);

		} catch (Exception e) {
			e.printStackTrace();
		}

	}

	
	public String doPosTagging(final String content) {
		try {
			ObjectStream<String> lineStream = new PlainTextByLineStream(
					new StringReader(content));

			String line;
			while ((line = lineStream.read()) != null) {

				String whitespaceTokenizerLine[] = WhitespaceTokenizer.INSTANCE
						.tokenize(line);
				String[] tags = posTagger.tag(whitespaceTokenizerLine);

				POSSample sample = new POSSample(whitespaceTokenizerLine, tags);

				return sample.toString();
			}
		} catch (Exception e) {
			e.printStackTrace();
		}

		return "";

	}


###################################################################################################################

public List<String> getNPChunks(final String content) {

		List<String> nplist;
		List<String> chunklist = new ArrayList<String>();
		try {
			nplist = chunkQtag(content);
			chunklist = proning(nplist);
		} catch (Exception e) {
			e.printStackTrace();
		}
		return chunklist;
	}
	
	public  List<String> chunkQtag(final String content)
			{
		List<String> result = new ArrayList<String>();
		SentenceSplitterImpl openSen = new SentenceSplitterImpl();

		List<String> sentList = openSen.splitTextIntoSentences(content);
		PosTaggerImpl openPos = new PosTaggerImpl();

		for (String sen : sentList) {
			String sub_sen[] = null;
			if (sen.contains(",")) {
				sub_sen = sen.split(",");
			} else {
				sub_sen = new String[1];
				sub_sen[0] = sen;
			}
			for (String st : sub_sen) {

				String s = openPos.doPosTagging(st);
				String lastTag = null;
				String lastToken = null;
				StringBuilder accum = new StringBuilder();

				for (String token : s.split(" ")) {
					String[] s2 = token.split("_");
					if (s2.length < 2) {
						continue;
					}
					String tag = s2[1];

					if (tag.equals("JJ") || tag.startsWith("NN")) {

						if ((lastTag != null && lastTag.startsWith("CC"))
								|| (lastToken != null && (lastToken
										.endsWith("of") || lastToken
										.endsWith("of the")))) {

							accum.append(lastToken).append(" ").append(s2[0])
									.append(" ");

						} else {
							accum.append(s2[0]).append(" ");
						}
						lastTag = tag;
						lastToken = s2[0];

					} else if (tag.equals("CC")
							&& lastTag != null
							&& (lastTag.startsWith("NN") || lastTag
									.startsWith("JJ"))) {
						lastTag = tag;
						lastToken = s2[0];

					} else if (lastTag != null && lastTag.startsWith("NN")
							&& s2[0].equalsIgnoreCase("of")) {
						lastTag = tag;
						lastToken = s2[0];

					} else if ((lastToken != null
							&& lastToken.equalsIgnoreCase("of") && s2[0]
								.equalsIgnoreCase("the"))) {
						lastTag = tag;
						lastToken = lastToken + " " + s2[0];

					}
					else {

						if (accum.length() > 0) {
							accum.deleteCharAt(accum.length() - 1);
							result.add(accum.toString());
							accum = new StringBuilder();
						}
						lastTag = "";
						lastToken = "";
					}

				}
				if (accum.length() > 0) {
					accum.deleteCharAt(accum.length() - 1);
					result.add(accum.toString().trim());
				}
			}
		}
		System.out.println(result);
		return result;

	}


	private List<String> proning(final List<String> nplist) {

		List<String> chunklist = new ArrayList<String>();
		for (String s : nplist) {

			if (s.trim().contains(" ") && !(s.endsWith("of"))) {

				if (s.trim().endsWith(".")) {
					s = s.replace(".", "");
				}

				if (s.split(" ").length < 5) {
					chunklist.add(s);
				}
			}

		}

		return chunklist;

	}
