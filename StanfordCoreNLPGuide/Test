Text Analytics Techniques and Applications

Abstract

Computer doesn’t understand human languages. It can't process and extract useful patterns from the enormous amount of unstructured information. Text Analytics refers to the process of extracting required information from the unstructured data. Therefore, text analytics is of huge interest for multiple organizations and academia for its various applications like sentiment analysis, summarization, text categorization, text clustering, concept/entity extraction and keyword extraction etc. In this paper, we will be discussing fundamental methods of text mining which includes natural language possessing, information extraction and machine learning. Also, we will cover the advantages and challenges of text mining.



Key Words

Text Analytics, Text mining, Natural language processing, Machine learning, Information retrieval;



Paper Type

Research and Technical 
 
1.	Introduction: 

Nowadays, most of information saved in companies are as unstructured models. Retrieval and extraction of the information is essential works and importance in semantic web
areas. Many of these requirements will be depend on the storage efficiency and unstructured data analysis. Merrill Lynch recently estimated that more than 80% of all potentially
useful business information is unstructured data. The large number and complexity of unstructured data opens up many new possibilities for the analyst. We analyze both structured
and unstructured data individually and collectively. Text mining and natural language processing are two techniques with their methods for knowledge discovery form textual
context in documents. In this study, text mining and natural language techniques will be illustrated. The aim of this work comparison and evaluation the similarities and differences
between text mining and natural language processing for extraction useful information via suitable themselves methods. Data Mining is knowledge detection and resolution
process of databases [3, 6]. It has obtained previously unknown, secret, meaningful and useful patterns being automatically established from large scaled databases [14,
15]. So, data mining knowledge discovery in databases is looking for patterns in data [10]. Likewise, text mining looking for patterns in text.

---

Text is unstructured, amorphous and complex form of data representation. Text is used for the formal exchange of information.  Conversation or interaction takes place by means of text written in natural language.
Text mining is the process of extracting meaningful 'high-quality’ data in structured format from unstructured data to discover knowledge from raw data. [10] 'High quality’ data in terms of text mining can be defined as meaningful, relevant and important information. The term mining is used to express the idea of extracting valuable substance from raw material.
Text analytics, web analytics and web mining are all roughly interrelated with text mining.  Interestingly, text mining is a subfield of data mining that seeks to extract valuable new information from unstructured (or semi-structured) source [1].  Web mining can be defined as subfield of text mining. [4] Web Mining is the technique used to extract useful information from data gathered from internet. Web Mining techniques are used to extract information from internet. [2]
Text analytics can be applied to multiple verticals such as finance, insurance, media, and retail industries and oil and gas etc. We have analyzed data from Media (brand specific social media analytics), Health care (side effect of drugs on patient from Pharmaceutical articles), Legal (Search, Topic identification, document clustering).
Major companies have made clear moves showing the importance of text analytics. we have built a platform which provides all sort of text analytics solutions such as document sentiment analysis, keyword extraction, topic extraction, entity extraction, classification and clustering.  We have used both lexical and machine learning technologies for performing the text analytics work.  This platform also includes all kind of natural language processing techniques such as tokenization, sentence detection, parts-of-speech tagging etc.

In this paper, we have described text mining techniques or algorithms as a truly interdisciplinary method which is interrelated with natural language processing, machine learning and information extraction.


2.	Related research areas:

Text mining techniques are primarily focused on natural language processing, information extraction, information retrieval and machine learning domain. These techniques are used for unlocking the business value within the vast data assets.

Natural Language processing is mainly used for the interactions between computers and human (natural) languages. This technique helps computer to understand semantic structure of content and parse text syntactically using formal grammar and lexicon.  

Information extraction is the process of extracting specific required data or semantic information from free text written in natural language. A common approach for identifying required data is looking for the predefined sequences in text i.e., pattern matching.

Information retrieval is a relatively old research area where first attempts for automatic indexing where made in 1975 [12]. Information retrieval is the finding of documents contain answers to questions and not the finding of answers itself [13]. Information retrieval has gained huge interest with the rise of World Wide Web.

Machine learning is an area of artificial intelligence concerned with the development of techniques for building analytical model by analysis of training data set.  We need to gather training dataset and extract the features/words from the training dataset. The next step is validation on another dataset to estimate accuracy of the model. Model building is an iterative process and we keep changing the features till the time we have the best model for the existing dataset.



3.	Advantages of Text Analytics:

 In the recent era of semantic web, most of the information is saved as unstructured data.  Unstructured data refers to usually computerized information that either does not have a data model nor has one that is not easily usable by a computer program [3]. Unstructured/semi-structured data can exist in the form of any file type such as text, pdf, xml and html etc.  In the modern world, social media like Twitter, Facebook, blogs and forums is a rich source of business related information. Twitter is one of the most popular social media, where real-time opinions from millions of users are expressed constantly.  By analyzing twitter data we could extract following insights about company's products and services: understand market share trends, pillars of brand values, provide early warning for brand values at risk, predict complaints, best and worst aspects of your competition etc. 

One approach for analyzing the unstructured data is to apply brute force manual labor i.e., reading all written   communication – emails; watch the videos; listen to the audio recordings; looking through social media data like tweet stream, reviews, Facebook page likes and comments etc. This way somebody can manually extract important keywords and interpret the emotional impressions of data. It might be challenging for companies to listen continuously to conversation of customer about their business, competitors and suppliers. But if you don't keep your ear open for the feedback you can never improve your business. There can be drastic impact on the business when negative conversation takes place amongst the customer. That's why reviews or user-generated data is of growing interest and highly valuable resource for marketing team of multiple companies.

Initially, businesses tried to analyze the data manually for getting a vibe of how thing are going on. Merrill Lynch estimated that more than 80% of all potentially useful business information is unstructured data [3, 4]. So, it is nearly impossible to extract semantics of data manually. The huge volume and complexity of unstructured data opens up many new opportunities for the analyst.  Text analytics is the solution for converting unstructured text into structured data. Due to advancement of technologies, text analytics allows market researchers to gain insights from the majority of data collected in business.
Just recently, Facebook announced the availability of Topic Data which uses text analytics to reveal what audiences are saying on Facebook about events, brands, subjects and activities. Marketers use this information to build product roadmaps and make better decisions about their activities. [5]




4.	Text Mining tools:

In NLP document processing for knowledge discovery consist of document pre-processing and Parameterisation. This approach is based on the application of different
techniques and rules that explicitly encode linguistic knowledge [30]. In document pre-processing level fundamentally consisting in preparing the documents for its
parameterisation, eliminating any elements considered as superfluous and in parameterisation level is a stage of minimal complexity once the relevant terms have been
identified[16]. With NLP techniques, the documents are analysed through different linguistic levels by linguistic tools that incorporate each level's own annotations to the text [29].
After having identified and analysed the words in a text, the next step is to see how they are related and used together in making larger grammatical units, phrases and sentences [22,
24]. 
---------------

There is a plethora of open source software to perform basic text mining processes. Stanford Natural Language Processing group provide a widely used, integrated NLP toolkit, Stanford CoreNLP. The Apache OpenNLP library is a machine learning based toolkit for the processing of natural language text.
The Natural Language Toolkit, or more commonly NLTK, is a suite of libraries and programs for symbolic and statistical natural language processing (NLP) for English written in the Python programming language. 
All these tools support the most common NLP tasks, such as tokenization, sentence segmentation, part-of-speech tagging, named entity extraction, chunking, parsing, and coreference resolution. These tasks are usually required to build more advanced text processing services. 
There exists a particular framework and development environment for text mining, called General Architecture for Text Engineering or GATE [Cunningham, 2002].  It provides support not just for standard text mining applications such as information extraction, but also for tasks such as building and annotating corpora, and evaluating the applications. Patterns can be specified by giving a particular text string, or annotations that have previously been created by modules such as the tokenizer, gazetteer, or document format analysis. [13]


5.1.	 Data Gathering:

Existing offline data (txt, doc, pdf etc.) or specify keywords for crawling required data from web sources like twitter, wikipedia or google data etc. Most of the major social media sites offer API access to external developers. We have been using open source python libraries such scrapy, beautifulSoup for crawling and scraping keyword specific documents. GNIP is Twitter's enterprise API platform and is used for gathering real-time and historical social data. . At this point, data is considered as raw data.

5.2.	Preprocessing:


In preprocessing step, the raw data will be processed to provide a platform for data analysis. The main purpose of this step is to classify raw sentences into a machine readable form. Generally, this machine readable form is an Attribute-Value table. This approach is rather expensive in terms of storage needed for entire attributes of characteristics of the document. However, a carefully parsed Attribute-Value table is essential for precise interpretation [2]. After parsing the raw content, we follow multiple preprocessing steps like tokenization, sentence splitting, pos tagging and stemming etc. Given below are brief description of each preprocessing steps:

5.2.1.	Tokenization: 
 
The tokenizer splits the text into very simple tokens such as numbers, punctuation and words. The common separator for identifying individual words is whitespace; however other symbols can also be used. Ending point of a word and beginning of the next word is called word boundaries. Tokenization is also known as word segmentation. For example, for this given sentence "I am Maitreyee" we get 3 tokens after tokenization i.e., "I", "am", "Maitreyee". List of tokens become input for further text mining processes.


5.2.2.	Sentence splitting:

The sentence splitter segments the text into sentences. It is also known as Sentence boundary disambiguation (SBD) or sentence breaking. It is the problem in natural language processing of deciding where sentences begin and end. Often natural language processing tools require their input to be divided into sentences for a number of reasons. However sentence boundary identification is challenging because punctuation marks are often ambiguous. In English and some other languages, using punctuation, particularly the full stop/period character is a reasonable approximation [17]. For example: "I am Maitreyee. I am from Kolkata." If we split the above text, we get 2 sentences.

5.2.3.	Part-of-speech tagging: 

Given a sentence, the process of part-of-speech tagging allows to automatically tag each word of text in terms of part of speech it belongs to: noun, pronoun, adverb, adjective, verb, interjection etc. This process is also known as POS-tagging, or simply tagging. Many words can have multiple tags such as both noun and verb. Part-of-Speech (POS) tagger programs use combinations of several techniques such as lexicons, rules, and dictionaries [17]. Dictionaries contain categories of words. Usually, tagging programs accurately tag the word or make a best guess. [8] When the words are ambiguous in a sentence, POS taggers use probability approaches to tag correctly. For example, “She is beautiful” sentence can be tagged as “She”: pronoun (PRP), “is”: Verb (VBZ), “beautiful”: Adjective (JJ)

5.2.4.	Stemming:

It is an automated process which produces a base string in an attempt to represent related words. In linguistic morphology and information retrieval, stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form i.e., generally a written word form. The stem need not be identical to the morphological root of the word. A stemming algorithm reduces the words "fishing", "fished", and "fisher" to the root word, "fish". [17]


5.2.5.	Lemmatization:

It can be defined as the process of determining lemma for a given word. This process involves understanding of context and determining parts of speech of a word.

Lemmatization is closely related to stemming. The difference is that a stemmer operates on a single word without knowledge of the context, and therefore cannot discriminate between words which have different meanings depending on part of speech. However, stemmers are typically easier to implement and run faster, and the reduced accuracy may not matter for some applications. For example, in English, the verb 'to walk' may appear as 'walk', 'walked', 'walks', 'walking'. The base form, 'walk', that one might look up in a dictionary, is called the lemma for the word. [17]

Therefore, the goal of both stemming and lemmatization is to "normalize" words to their common base form, which is useful for many text-processing applications.


5.2.6.	Stop-word Removal:

As a part of preprocessing we need to get rid of unwanted words. These high-frequency words are non-contextual words occurring in the text. They do not directly contribute to the content. Therefore, they are listed in the stop-list. It is clear that those words appearing in the stop-list will be deleted from the set of characteristic words before concept extraction. 


5.2.7.	Word sense disambiguation:

Many words have more than one meaning. A human can understand the meaning of a word based on the prior knowledge of the subject and the context it is spoken.  But, machine would have difficulties in identifying such proper meaning.
Word-sense Disambiguation is a NLP technique which helps to identify the right sense of the ambiguous word in a sentence. This process helps a machine to minimize the ambiguities of words in the text. In general, a word-sense disambiguation approach consists of a lexical repository that contains different senses for words [6]. WordNet is a free lexical database in English that contains a large collection of words and senses [7]. The design of WordNet was inspired by the theories of human linguistic memory [18]. WordNet encloses a large volume of nouns, verbs, adjectives and adverbs in English language. In WordNet, words are grouped and interlinked by their meanings. This method allows the identification of any closer or similar meaning to a given word. Many NLP applications use WordNet as a source for processing.

. In this paper, main focus is to present the text analytics algorithms and their use in a simplest manner. 

By using NLP techniques we can extract [12] several types of information:- 

5.3.1.	Named Entity Extraction: 

It is the process of extracting small chunks/fragments (people, place, time, date, address) of text by matching patterns. Named Entity Recognition is the subject area that identifies entities or physical objects such as names of persons, organizations, and places in the text. For an example, Named Entity recognition of sports news would consist of names of the players, places of teams and grounds, etc. Traditionally, grammar based approaches are used to identify name and entities in the text. In the present systems, statistical models are incorporated in order to classify names and entities more precisely. [2]





5.3.2.	Keyword Extraction:

Keyword extraction task is an important problem in Text Mining, Information Retrieval and Natural Language Processing. It is the process of automatic identification of most significant words and phrases that best describe the subject of a document.

It is one of the most important tasks when dealing with text. By using this algorithm, dimensionality of the text is reduced to the most important features. There are varieties of application of this algorithm. Given below are examples of few applications:

•	Reader can be highly benefited as they can quickly look through the important keywords of the document and decide whether the text is worth reading or not.
•	 Website creator can also be benefited as they can group similar content by keywords.

5.3.3.	Automatic Summarization: 

Summarization is one of the most important techniques. It is the process of bringing out main idea behind a large document. It helps to reduce a large text into a meaningful short paragraph. It helps the user to understand whether a particular document is useful for him or not [10]. For an example of summarization, ‘abstract’ part of a research paper describes the main idea of research conducted by author(s) [11] In general, there are two types of approaches used in Automatic Summarization. One is Extraction which extracts few important parts from the text such as key word sentences or paragraphs. Another type of approach is Abstraction which paraphrases the important points of the text. Technically, Abstraction is a more complicated system than Extraction to develop. [9]
For this operation, users are allowed to define a number of parameters such as the number of sentences to extract or a percentage of the total text to extract.






5.3.4.	Sentiment analysis:

Reviews or user-generated data is of growing interest and highly valuable resource for marketing team of multiple companies. Social listening from social media like Twitter, Facebook, blogs and forums is a rich source of business related information. Twitter is one of the most popular social media, where real-time opinions from millions of users are expressed constantly. 
Sentiment analysis is a line of research that allows determining people's attitude or opinion or contextual polarity in relation to different topics, products, services, events, and their attributes. It classifies text written in a natural language into positive, negative or neutral about a given subject. We could perform sentiment analysis by using lexicon and machine learning approach. In machine learning approach, we have used Naïve Bayes classifier for sentiment classification. The Naïve Bayes classifier is the simplest and most commonly used classifier.



5.3.5.	Document classification:

The main aim of text categorization is to classify a set of documents into a fixed number of predefined categories. Each document may belong to more than one class. This process involved identification of main themes of the document by placing the document into pre-defined set of topics. For this task, document is treated as “bag of words” by computer. In this case, actual information is not processed like information extraction. It relies on thesaurus and relationships are identified by looking broad terms, narrow terms, synonyms etc. Tools designed for this purpose ranks the document on the basis of order in which document has most content on particular topic Using supervised learning algorithms [10], the objective is to learn classifiers from known examples (labeled documents) and perform the classification automatically on unknown examples (unlabeled documents).
 


5.3.6.	Document clustering:

Clustering is the most common unsupervised learning method. It helps to organize large collections of uncategorized documents by automatically grouping them based on similarity measure. A general definition of clustering stated by Brian Everitt et al. [16]
Given a number of objects or individuals, each of which is described by a set of numerical measures, devise a classification scheme for grouping the objects into a number of classes such that objects within classes are similar in some respect and unlike those from other classes. The number of classes and the characteristics of each class are to be determined.

Document clustering has multiple applications in the area of text mining and information retrieval.  Given below are few of the applications of clustering:
•	Recommendation system is one of the very important applications of document clustering. Clustering technique helps to find similar documents which are conceptually alike. In this application, user is recommended similar article based on his reading history.
•	Clustering technique has huge impact on the quality and efficiency of search engine. Users search query can be compared with the cluster of documents based on the label of cluster. This way search result will be faster as we don't need to compare with all the documents.

5.3.7.	Theme extraction:

Theme extraction is the process of extracting actual context of the document. Themes are basically noun phrases with contextual relevance. Noun phrases are extracted by applying parts-of-speech patterns on text. We consider noun phrase as theme because nouns are most useful to understand the context of a text.

We can generate word-cloud based on the theme output. As these phrases bring actual context and meaning of the text, so it’s always worth to show these result in the form of a word-cloud.

5.4.	Search and Retrieval:

Once data is extracted we need to index the data to be queried. The efficiency and performance of the program depends on the type of indices chosen in this step. Current search engines do this very efficiently. When choosing the appropriate index method; storage types, look up speeds and fault tolerance must be considered. These factors are different for each industry and organization. Cost of storage and speed of retrieval are always a concern in any commercial application. In many situations, in order to reduce storage cost few indexes are created. Even though this approach reduces the cost, it increases the possibility of fault occurrences. [2]

Inverted index is the most common index method used in text retrievals. In inverted indexing, lists are made from terms that appear in the text collection [15]. A simple example of inverted indexing is the index published at the end of books where each word and page numbers are given.



5.3.	Feature Processing:

Required information is extracted at this stage. Information Extraction utilizes relations within the text. It uses pattern matching for it [11]. Extracting meaningful information from unstructured data is not an easy job to do. By using natural language processing techniques, data can be converted into structured one. We get to know grammatical structure of the data by applying NLP techniques. IE process extracts important keywords by matching patterns. 
By using NLP techniques we can extract [12] several types of information:- 

5.3.1.	Named Entity Extraction: 

It is the process of extracting small chunks/fragments (people, place, time, date, address) of text by matching patterns. Named Entity Recognition is the subject area that identifies entities or physical objects such as names of persons, organizations, and places in the text. For an example, Named Entity recognition of sports news would consist of names of the players, places of teams and grounds, etc. Traditionally, grammar based approaches are used to identify name and entities in the text. In the present systems, statistical models are incorporated in order to classify names and entities more precisely. [2]





5.3.2.	Keyword Extraction:

Keyword extraction task is an important problem in Text Mining, Information Retrieval and Natural Language Processing. It is the process of automatic identification of most significant words and phrases that best describe the subject of a document.

It is one of the most important tasks when dealing with text. By using this algorithm, dimensionality of the text is reduced to the most important features. There are varieties of application of this algorithm. Given below are examples of few applications:

•	Reader can be highly benefited as they can quickly look through the important keywords of the document and decide whether the text is worth reading or not.
•	 Website creator can also be benefited as they can group similar content by keywords.

5.3.3.	Automatic Summarization: 

Summarization is one of the most important techniques. It is the process of bringing out main idea behind a large document. It helps to reduce a large text into a meaningful short paragraph. It helps the user to understand whether a particular document is useful for him or not [10]. For an example of summarization, ‘abstract’ part of a research paper describes the main idea of research conducted by author(s) [11] In general, there are two types of approaches used in Automatic Summarization. One is Extraction which extracts few important parts from the text such as key word sentences or paragraphs. Another type of approach is Abstraction which paraphrases the important points of the text. Technically, Abstraction is a more complicated system than Extraction to develop. [9]
For this operation, users are allowed to define a number of parameters such as the number of sentences to extract or a percentage of the total text to extract.






5.3.4.	Sentiment analysis:

Reviews or user-generated data is of growing interest and highly valuable resource for marketing team of multiple companies. Social listening from social media like Twitter, Facebook, blogs and forums is a rich source of business related information. Twitter is one of the most popular social media, where real-time opinions from millions of users are expressed constantly. 
Sentiment analysis is a line of research that allows determining people's attitude or opinion or contextual polarity in relation to different topics, products, services, events, and their attributes. It classifies text written in a natural language into positive, negative or neutral about a given subject. We could perform sentiment analysis by using lexicon and machine learning approach. In machine learning approach, we have used Naïve Bayes classifier for sentiment classification. The Naïve Bayes classifier is the simplest and most commonly used classifier.



5.3.5.	Document classification:

The main aim of text categorization is to classify a set of documents into a fixed number of predefined categories. Each document may belong to more than one class. This process involved identification of main themes of the document by placing the document into pre-defined set of topics. For this task, document is treated as “bag of words” by computer. In this case, actual information is not processed like information extraction. It relies on thesaurus and relationships are identified by looking broad terms, narrow terms, synonyms etc. Tools designed for this purpose ranks the document on the basis of order in which document has most content on particular topic Using supervised learning algorithms [10], the objective is to learn classifiers from known examples (labeled documents) and perform the classification automatically on unknown examples (unlabeled documents).
 


5.3.6.	Document clustering:

Clustering is the most common unsupervised learning method. It helps to organize large collections of uncategorized documents by automatically grouping them based on similarity measure. A general definition of clustering stated by Brian Everitt et al. [16]
Given a number of objects or individuals, each of which is described by a set of numerical measures, devise a classification scheme for grouping the objects into a number of classes such that objects within classes are similar in some respect and unlike those from other classes. The number of classes and the characteristics of each class are to be determined.

Document clustering has multiple applications in the area of text mining and information retrieval.  Given below are few of the applications of clustering:
•	Recommendation system is one of the very important applications of document clustering. Clustering technique helps to find similar documents which are conceptually alike. In this application, user is recommended similar article based on his reading history.
•	Clustering technique has huge impact on the quality and efficiency of search engine. Users search query can be compared with the cluster of documents based on the label of cluster. This way search result will be faster as we don't need to compare with all the documents.

5.3.7.	Theme extraction:

Theme extraction is the process of extracting actual context of the document. Themes are basically noun phrases with contextual relevance. Noun phrases are extracted by applying parts-of-speech patterns on text. We consider noun phrase as theme because nouns are most useful to understand the context of a text.

We can generate word-cloud based on the theme output. As these phrases bring actual context and meaning of the text, so it’s always worth to show these result in the form of a word-cloud.

5.4.	Search and Retrieval:

Once data is extracted we need to index the data to be queried. The efficiency and performance of the program depends on the type of indices chosen in this step. Current search engines do this very efficiently. When choosing the appropriate index method; storage types, look up speeds and fault tolerance must be considered. These factors are different for each industry and organization. Cost of storage and speed of retrieval are always a concern in any commercial application. In many situations, in order to reduce storage cost few indexes are created. Even though this approach reduces the cost, it increases the possibility of fault occurrences. [2]

Inverted index is the most common index method used in text retrievals. In inverted indexing, lists are made from terms that appear in the text collection [15]. A simple example of inverted indexing is the index published at the end of books where each word and page numbers are given.



5.5.	 Visualization: 

According to David McCandless, "By visualizing information we turn it into a landscape that you can explore with your eyes, a sort of information map. And when you’re lost in Inform, an information map is kind of useful." [19]

After extracting the required structured information from unstructured data, we need to display the data to the user in a presentable and meaningful manner.  It is easy to understand and visually analyze the data if it is mapped properly. The user can interact with the document map by zooming, scaling, and creating sub-maps. Information visualization is useful when a user needs to narrow down a broad range of documents and explore related topics.  Visual text mining or information visualization is a process where the vast volume of text is processed and provides browsing capabilities. [10]


6.	Conclusion:

This paper has presented an overview of text analytics, with its techniques, approach and applications. We have discussed step by step process of text mining for building an intelligent system. Also, this paper explains the interaction of text mining with other fields, especially with natural language processing and machine learning. Text analytics techniques and its importance is growing rapidly but still it has long way to go in the position to challenge human capabilities. In this study, an attempt has been made to implement basic platform which can provide all sorts of text mining functionalities. In future, we can extend this work by implementing new business rules or new machine learning algorithm based on the dataset and requirement.







7.	References:

[1] Feldman R, Sanger J. The Text Mining Handbook: Advanced Approaches in Analyzing Unstructured Data. Cambridge University Press, 2007
[2] John Selvadurai,"A  Natural Language Processing based  Web Mining System for Social Media analysis",International Journal of Scientific and Research Publications, Volume 3, Issue 1, January 2013.
[3] Y. Shiqun, Q. Yuhui, G. Jike and W. Fang,2008, “A Chinese Text Classification Approach Based on Semantic Web”, Fourth International
[4]Grimes, Seth. "A Brief History of Text Analytics". B Eye Network. Retrieved June 24, 2016.
[5] http://insidebigdata.com/2015/06/05/text-analytics-the-next-generation-of-big-data/
[6] S. Bandyopadhyay,  S. Naskar and A. Ekbal. Emerging Applications of Natural Language processing, Hershey, PA, USA: IGI Global, 2013.
[7]Princeton University. (2012, Nov 10). WordNet: A Lexical database for English [Online]. Available: http://wordnet.princeton.edu/
[8]Robin, (2012, Nov 12). Natural Language Processing: Parts-of-speech tagging,[Online].       
Available: http://language.worldofcomputing.net/pos-tagging/parts-of-speech-tagging.html
[9] D.Bikel and I.Zitouni, Multilingual Natural Language Processing Applications: From Theory to Practice, USA: IBM Press, 2012, pp. 400.
[10] Vishal Gupta and Gurpreet S. Lehal, “A Survey of Text Mining Techniques and Applications” Journal of Emerging Technologies in Web Intelligence, Vol. 1, No. 1, August 2009.
[11] Abhishek Kaushik† and Sudhanshu Naithani, "A Comprehensive Study of Text Mining Approach", IJCSNS International Journal of Computer Science and Network Security, VOL.16 No.2, February 2016.
[12] Goutam Chakraborty, Murali Pagolu and Satish Garla, “Text Mining and Analysis: Practical Methods, Examples, and Case Studies Using SAS”.
[13] Ian H. Witten,"Text mining", Computer Science, University of Waikato, Hamilton, New Zealand.
 [14] H. Prado and E. Ferneda, "Emerging Technologies of Text Mining: Techniques and Application", Hershey, PA, USA: IGI Global, 2009, pp. 56.
[15] J. Lin and C. Dyer, Data-Intensive Text Processing with MapReduce, USA: Morgan and Claypool, 2010.
[16]Brian S. Everitt, Sabine Landau, and Morven Leese. Cluster Analysis. Oxford University Press, fourth edition, 2001
[17]https://en.wikipedia.org
[18] M. Song and Y. Wu, Handbook of Research on Text and Web Mining Technology, Hershey, PA, USA: IGI Global, 2009, pp. 194.
[19] White Paper on “Turning Big Data into Big Insights: The Rise of Visualization-based Data Discovery Tools” sponsored by intel, March 2013.
----------------------------------











The Need for Text Mining in Business Intelligence

Editor's Note: This article is featured in the 2001 Resource Guide, a supplement to the December issue of DM Review.
Although data warehouses are widely adopted, most fail to tap the business intelligence potential of text. To date, the focus has been on developing data warehouses geared to support primarily numeric data, and the payoff has been enormous. Enterprises now have at their disposal a suite of proven practices and methodologies along with mature tools for number-centric data warehousing. It is now time to focus on the business intelligence value of text and the role of text mining techniques in harnessing this relatively untapped source of business intelligence.
Why Bother with Text?
There are two primary reasons to take on the challenges of text for business intelligence. First, there is far too much critical information that remains inaccessible in documents. Business intelligence systems driven by data warehouses excel at telling us what happened when, but they are not very good at answering why. We can easily discover that a product's sales margins decreased by 15 percent in the last quarter in the southeast region without knowing the cause. Did a competitor release a higher quality, lower price alternative? Were the margins sacrificed on this product as part of a cross- selling campaign? Did the manufacturer license another distributor in the southeast, thus creating new competition?
The answers to these and other questions are buried in documents ranging from e-mails, status memos, news stories and press releases to complex documents such as marketing campaigns, contracts, regulatory agency filings and government reports. To extend the depth of business intelligence, text must be considered.
The second reason to address text directly is that traditional document and text management tools are inadequate to meet the demands of business intelligence. File systems provide crude searching and pattern matching utilities. Document management systems work well with homogeneous collections of documents but not with the heterogeneous mix that knowledge workers face every day. Even the best Internet search tools suffer from poor precision and recall. (Precision is a measure of how many documents returned from a search actually meet the intended query criterion. Recall measures the percentage of documents returned versus how many should have been returned.) Finally, documents are spread across platforms in different formats and languages with little useful meta data about the content of the documents. This same type of dispersion of data is a driving factor in the development of many data warehouses. Business intelligence users need, and have become accustomed to, an integrated view of their organization without regard to the original source or distribution of the raw data. Logically, text is just another medium for conveying information and, thus, belongs within the realm of business intelligence systems.
However, text is different. It is not structured like the numeric measures we are accustomed to dealing with. Or is it? Although text is often described as unstructured, that is far from the truth. Language is richly structured at multiple levels as linguists have aptly discovered. Structural principles are found in the formation of words (morphology), the creation of grammatical sentences (syntax) and the representation of meaning (semantics). Even higher levels of structure can be found in discourses and conversations as described by speech act theory. If we can analyze the structure of language, we can extract the information conveyed by text. Fortunately, after decades of foundational work in computational linguistics, tools are now available to delve into the complex structures of text and extract vital business information. 
Text Mining: The Basics
Text mining is the study and practice of extracting information from text using the principles of computational linguistics. Certainly, AWK, grep and other pattern matching tools can extract information from text files, but these do not fall within the realm of text mining tools. For our purposes, the key areas of text mining include:
•	Feature extraction
•	Thematic indexing
•	Clustering
•	Summarization
These four techniques are essential because they solve two key problems with using text in business intelligence: they make textual information accessible, and they reduce the volume of text that must be read by end users before information is found.
Feature extraction deals with finding particular pieces of information within a text. The target information can be of a general form such as type descriptions or business relationships. Identifying Alpha Industries as a corporation is an example of the former, while Alpha Industries, a wholly owned subsidiary of Beta Enterprises, and Margaret Johnson, president and CEO of Gamma Group, Inc., are examples of business relationships. Feature extraction can also be pattern-driven. For example, applications analyzing merger and acquisition stories may extract names of the companies involved, cost, funding mechanisms and whether or not regulatory approval is required. 
Thematic indexing uses knowledge about the meaning of words in a text to identify broad topics covered in a document. For example, documents about aspirin and ibuprofen might be both classified under pain relievers or analgesics. Thematic indexing such as this is often implemented using multidimensional taxonomies. A taxonomy, in the text mining sense, is a hierarchical knowledge representation scheme. This construct, sometimes called ontology to distinguish it from navigational taxonomies such as Yahoo's, provides the means to search for documents about a topic instead of documents with particular keywords. For example, an analyst researching mobile communications should be able to search for documents about wireless protocols without having to know key phrases such as wireless application protocol (WAP).
Clustering is another text mining technique with applications in business intelligence. Clustering groups similar documents according to dominant features. In text mining and information retrieval, a weighted feature vector is frequently used to describe a document. These feature vectors contain a list of the main themes or keywords along with a numeric weight indicating the relative importance of the theme or term to the document as a whole. Unlike data mining applications which use a fixed set of features for all analyzed items (e.g. age, income, gender, etc.), documents are described with a small number of terms or themes chosen from potentially thousands of possible dimensions. For example, a news story about Malaysia trade policies might a feature vector as illustrated in Figure 1. Figure 2 provides an example of a feature vector for an article about the Euro. 
Intl. Trade	Tariffs	Southeast Asia	Currency Exchange	Shipping	Industrial Manufacturing
0.94	0.91	0.89	0.84	0.76	0.64
Figure 1: Feature Vector for Story on Malaysian Trade Policies
Although the two vectors share a dimension in common, most are different. The result is that unlike the relatively dense dimensional models in OLAP applications, dimensional models for documents are extremely sparse. 
Currency Exchange	European Union	U.S. Dollar	Japanese Yen	Trade Deficit	Equity Markets
0.96	0.93	0.54	0.52	0.48	0.23
Figure 2: Feature Vector for Article on Euro
There is no single, best way to deal with document clustering; but three approaches are often used: hierarchical clusters, binary clusters and self-organizing maps. Hierarchical clusters use a set-based approach. The root of the hierarchy is the set of all documents in a collection, and the leaf nodes are sets with individual documents. Intervening layers in the leaf nodes have progressively larger sets of documents, grouped by similarity. Binary clusters are similar to k-NN clusters in data mining. Each document is in one and only one cluster, and clusters are created to maximize the similarity measures between documents in a cluster and minimize the similarity measure between documents in different clusters. Self-organizing maps (SOMs) use neural networks to map documents from sparse high-dimensional spaces into two-dimensional maps. Similar documents tend to the same position in the two dimensional grid.
The last text mining technique is summarization. The purpose of summarization is to describe the content of a document while reducing the amount of text a user must read. The main ideas of most documents can be described with as little as 20 percent of the original text. Little is lost by summarizing. Like clustering, there is no single summarization algorithm. Most use morphological analysis of words to identify the most frequently used terms while eliminating words that carry little meaning, such as the articles the, an and a. Some algorithms weight terms used in opening or closing sentences more heavily than other terms, while some approaches look for key phrases that identify important sentences such as in conclusion and most importantly.
With these techniques in hand, it is time to turn to the issue of integrating text in the data warehouse.
Extending the Warehouse
Extending the data warehouse to support documents and text mining will require new data structures as well as new tools.
Accommodating text in the warehouse requires support for the text itself along with its meta data. Storing documents is not a problem for RDBMSs that support binary large objects. Some, such as Oracle8i, provide direct support for documents in the warehouse. 
Documents are meta data- intensive objects. In general, the data warehouse should support meta data about document source, analysis and content. Source meta data describes where a document originated, when it was loaded along with quality and timeliness information. Analysis meta data drives the type of text mining performed on documents. For example, e-mails should not be summarized, but they are good candidates for clustering using self-organizing maps. Content meta data should include at least the attributes delineated in the Dublin Core, a meta data standard for Internet resources. The Dublin Core includes title, creator, subject, description, dates of publication, copyrights, format and relationships to other works. Content meta data will also include information mined during text analysis, such as features and business relationships mentioned in the text.
Working with text will require additional tools. Although some features are built into database systems, additional functionality is needed to take full advantage of text mining. IBM Intel-ligent Miner for Text (www.ibm.com) includes summarization, document clas-sification and clustering tools. Oracle Intermedia Text (www.oracle.com) provides thematic indexing and summarization right in the database. Specialty tools, such as Megaputer's Text Analyst (www.megaputer.com) provides text mining functionality through COM objects for custom-built applications. Semio's (www.semio.co) taxonomy-generation tool can be used to automate the creation of ontologies while Mohomine's (www.mohomine.com) tool suite includes Web crawlers and document classifiers. Of course, it is the end user's needs that will ultimately drive the set of tools required for a particular application.
Text: The Next Dimension
If business information were an iceberg, text would be bulk of the glacial object hidden below the surface and usually forgotten. Fortunately, things are changing. Commercial quality text mining tools are available, and database vendors are recognizing the need to manage text along with numeric data. The Internet provides a wealth of raw material to complement internal documents. Whether a user needs to understand why an anomalous pattern is showing up in the data warehouse, monitor market conditions or conduct competitive intelligence research, text is central to meeting those business intelligence needs. The time has come to accommodate documents within the workhorse of business intelligence – the data warehouse.












------------------------------



Abstract

Sentiment analysis is a line of research that allows determining people's attitude and opinions in relation to different topics, products, services, events, and their attributes. It classifies text written in a natural language into positive or negative about a given subject. 

In this paper we discuss on the various methods used for classifying a given piece of natural language text based on the sentiment expressed in it. We have done sentiment analysis on top of twitter data in our Genpact Media Interactive (GMI) app. We had classified the tweets as negative or positive.

Key Words

Text analytics, Text classification, Sentiment analysis, Naïve Bayes;

Paper Type

Research and Technical 
 
1.	Introduction: 


Reviews or user-generated data is of growing interest and highly valuable resource for marketing team of multiple companies. Social listening from social media like Twitter, Facebook, blogs and forums is a rich source of business related information. Twitter is one of the most popular social media, where real-time opinions from millions of users are expressed constantly. By analyzing twitter data we could extract following insights about company's products and services: understand market share trends, pillars of brand values, provide early warning for brand values at risk, predict complaints, best and worst aspects of your competition etc.
This paper mainly focuses on different methods of classifying a piece of natural language text based on the mood or opinion of subjective elements within a text. We will be discussing lexicon based approach and machine learning based approach.  


2.	 Related Work:

The two main areas of research in sentiment classification are lexical and machine learning approaches. For the lexical approach, a domain dictionary is prepared to store the polarity values of lexicons. Overall polarity score is calculated based on the polarity of each word. If word matches positive lexicon or negative lexicon then positive score or negative score of the text is increased. If the overall polarity score of a text is positive, then that text is classified as positive, otherwise it is classified as negative. This approach has been reported to have considerably high accuracy. [1].

Since the polarity of the text depends on the score given to each lexicon, there has been a large volume of work dedicated to discovering which lexical information is most efficient. Hatzivassiloglou and Wiebe1 reported accuracy of over 80% using hand tagged lexicons comprising solely of adjectives to evaluate the subjectivity of the sentence. Kennedy and Inkpen [2] however, utilizing the same methodology, reported a much lower accuracy rate of about 62% on a dataset composed of movie reviews. Turney [3] determined the polarity of words using an Internet search engine. Turney [3] performed two AltaVista search engine queries: one with a target word along with the word `good', and a second with the target word with the word `bad'. The polarity of the target word was determined by the search result that returned the most hits. This approach resulted in accuracy of 65%. Kamps et al. [4] and Andreevskaia et al. [5] used the WordNet database to determine the polarity of words. They compared a target word to two pivot words (usually `good' and `bad') to find the minimum path distance between the target word and the pivot words in the WordNet hierarchy. The minimum path distance was converted to an incremental score and this value was stored with the word in the dictionary. The reported accuracy level of this approach was 64% [6]. Turney and Littman on the other hand mapped the semantic association between the target word and each word from the selected set of positive and negative words to a real number. By subtracting a word's association strength to a set of negative words from its association strength to a set of positive words, an accuracy rate of 82% was achieved.

For the machine learning approach, a series of feature vectors are chosen and a collection of tagged corpora is used to prepare a model. The model can then be used to classify an untagged corpus of text. In machine learning approach, the selection of features is crucial to the success rate of the classification. Most commonly, a variety of unigrams (single words from a document) or n-grams (two or more words from a document in sequential order) are chosen as feature vectors. Most commonly employed classification techniques of machine learning are Support Vector Machines (SVMs), Naive Bayes and Maximum Entropy algorithm. The accuracy results for these algorithms greatly depends on the features selected [7].








3.	Sentiment Analysis Methods:

There are many different approaches being practiced in the Industry to determine the polarity of text such as Lexicon/Rule based approach and Machine learning based approach. We will understand the bit of each and concentrate mainly on the machine learning based approach. We will look at bunch of details on what kind of training data we need, what are the different features that we could use for creating a classification model. 

3.1.	Lexical Method: 

Lexical method or rule based approaches require you to study vocabulary and a lot of lexical patterns, features and so on. Rules are defined based on what are the words presents in that particular text, what are the relationships between the words in that text and so on. These set of rules determine what is the polarity of a particular document. 

Counting the number of the positive or negative words is the simplest rule based approach that you can think of. For this you would need something called as lexicon. A Lexicon is a resource which contains predefined list of words where all words which we would ever encounter have been classified as positive or negative.

It starts with few preprocessing steps like ‘tokenization’, ‘sentence splitting’ and ‘pos tagging’ before matching the words with the dictionary. We iterate through the set of tokens of each sentences to check whether any token with pos tagging of adjective/noun/adverb/verb is present in the lexicon.
 The score is updated according to which lexicon the word is present. The final score will classify the text as positive or negative.

     Some variants of the lexical approach are:

3.1.1.	Tokenization:  

It is the task of chopping the text into tokens. The tokens may be words or number or punctuation mark. Tokenization does this task by locating word boundaries. Ending point of a word and beginning of the next word is called word boundaries. Tokenization is also known as word segmentation. 
Example:  For this given sentence "I am Maitreyee" we get 3 tokens after tokenization i.e., "I", "am", "Maitreyee". 
 
3.1.2.	Sentence splitting: 

Sentence splitting is the process of dividing a string of text into its component sentences. In English and some other languages, using punctuation, particularly the full stop/period character is a reasonable approximation. [10]

Example:
"I am Maitreyee. I am Genpact employee."

If we split the above text, we get 2 sentences.

 
3.1.3.	Pos tagging: 

It is task of assigning one of the parts of speech to the given word. It is commonly referred to as POS tagging. Parts of speech include nouns, verbs, adverbs, adjectives, pronouns, conjunction and their sub-categories. [10]

Example:
She is beautiful.
She: pronoun (PRP), is: Verb (VBZ), beautiful: Adjective (JJ)


3.1.4.	 N-grams: 

N-gram is a contiguous sequence of n items from a given sequence of text or speech. The items can be phonemes, syllables, letters, words or base pairs according to the application. In our implementation, we have used unigram approach.


3.1.5.	 Conjunction Rules: 

Conjunction rules help us to extract the precise meaning or expression from a given sentence using grammar rules. Generally, a sentence only expresses one opinion orientation unless there is some conjunctions such as ‘but’, ‘although’, ‘however’, ‘while’ that changes the direction of the sentence. Conjunction rules explanations are shown as below: [1]

•	Although (Phrase A), (Phrase B).
E.g., “although new X brand mobile is nice, sadly it has short battery life”. In this case, phrase A can be cut off, and phrase B can be used for sentiment identification.

•	(Phrase A), but (Phrase B).
E.g., “the brand X mobile appearance is not beautiful, but very durable”. In this case, phrase A can be cut off, and phrase B can be used for sentiment identification.

•	Although (Phrase A), (Phrase B), but (Phrase C).
E.g. “although this brand X mobile is nice, too bad it has short battery life, but I still like it”. In this case, phrase A and B can be cut off, and phrase C can be used for sentiment identification. By applying conjunction rules, the sentences become more logical and clear-cut. Hemnaath and Low [6] proved that application of conjunction rules increases the accuracy of sentiment analysis by approximately 5%.


3.1.6.	 Stop Words:
Stop words are basically uninformative or unnecessary words for a particular work. Stop-words mainly include articles, prepositions, conjunctions and pronouns. Articles such as ‘a’ and ‘the’ and pronouns such as ‘he,’ ‘they,’ and ‘I’ provide little or no information about sentiment. We remove the stop-words to get rid of unwanted data. [1]




3.1.7.	 Negation method:
A negation word, such as ‘not’ inverts the evaluative value of an affective word. For example, ‘not bad’ is similar to saying ‘good’. In unigrams, the value of 'bad' is negative, but there is a negation word ‘not’ changes the polarity of the entire sentence.

3.2.	 Machine Learning Methods:

This technique requires creating a model by training the classifier with labeled examples. This means that you must first gather a dataset with examples for positive and negative classes, extract the features/words from the examples and then train the algorithm based on the examples. After training the classifier we can test on another set whether it is able to detect the right features and give the right classification.  
We have used Naïve Bayes classifier for sentiment classification. The Naïve Bayes classifier is the simplest and most commonly used classifier. Given below is the description of properties and advantages of using this classifier.
3.2.1.	 Naïve Bayes Algorithm:
It is a probabilistic classifier which uses the properties of Bayes theorem assuming the strong independence between the features. It uses Bayes Theorem to predict the probability that a given feature set belongs to a particular label. Naïve Bayes classification model computes the posterior probability of a class, based on the distribution of the words in the document. The model works with the BOWs feature extraction which ignores the position of the word in the document.
One of the advantages of this classifier is that it requires small amount of training data to calculate the parameters for prediction. Instead of calculating the complete covariance matrix, only variance of the feature is computed because of independence of features. [1]

The maximum accuracy was achieved by Yu, Hong and Vasileios Hatzivassiloglou [8] by using Naive Bayes, a commonly used supervised machine-learning algorithm. This approach presupposes the availability of at least a collection of articles with pre-assigned opinion and fact labels at the document level. They used single words, without stemming or stop-word removal as features. [1] Naive Bayes assigns a document d to the class c, that maximizes P (c | d) by applying Bayes’ rule,


                                P(c / d) = P(c) P (d / c) /P (d)

3.2.1.1.	Input: In our implementation, we have taken a csv file as input where sentiment text is tagged against sentiment category. 


3.2.1.2.	 Process: Labelled twitter data is read into Python environment followed by few data cleaning steps like removing special character etc. Post cleaning the twitter data, words are sorted based on the frequency of occurrence. The "feature words" needs to be assigned weights so that algorithm can assign more relative weights to those words while classifying sentiment of text. Presence and weightage of certain words define the sentiment class of the text/tweet. We have used Naive bays classifier for the sentiment classification work. During processing phase, the algorithms takes labelled data as input and after applying selective business rules, model is generated for classifying unlabeled data.


3.2.1.3.	Output: The output accuracy can be iteratively measured using “Confusion matrix” and model can be tuned accordingly to get improved results. Business rules can be implemented for improving the accuracy of the model.





4.	 Model Implementation using NLTK Python:

4.1.	Collect training data:

Gather a dataset with examples of positive and negative sentiment.  We have collected the training dataset in csv file where sentiment text is tagged against labelled sentiment (1 denotes positive sentiment and -1 denotes negative sentiment).
 
4.1.1.	  Load  and preprocess training data :
 
Labelled twitter data is read into Python environment followed by few data cleaning steps like removing special character etc. Uninformative part from the text is removed to improve the performance of the classifier and speed up the classification process. While reading the csv file iterate through each tweet and follow the given below steps. 
text= row['SentimentText']
line=re.sub('\s+', ' ', text).strip() #Remove multiple spaces
line=re.sub('[^A-Za-z0-9]+', ' ', line) #Remove special character




4.2.	Feature generation:

Capture the attributes of data which are the most useful and have a higher impact on the orientation of the text than other words in the same text in the context of opinion mining. [9] Filter the words by removing all the words of length lesser than 3. 
 
from nltk.probability import *
from nltk import NaiveBayesClassifier
from nltk import FreqDist, ConditionalFreqDist
import nltk.classify.util
from nltk.classify import NaiveBayesClassifier

def filterWords(data):
    tweets = []
    for (words, sentiment) in data:
        words_filtered = [e.lower() for e in words.split() if len(e) >= 3]
        tweets.append((words_filtered, sentiment))
    return tweets



4.3.	Feature Selection:

There are several methods that are used in feature selection.  We can assess the importance of each feature by attaching a certain weight in the text. 
Here we are using feature frequency (FF), and feature presence (FP) for feature selection. Features need to be extracted from the tweets for building a classifier.  There are several ways to assess the importance of each feature by attaching a certain weight in the text. In our implementation, we have considered “Feature Presence” and “Feature Frequency” for selecting most relevant feature.  

‘word_features’ list contains every distinct words ordered by frequency of appearance.   If we print inside the function ‘get_word_features’, the variable ‘wordlist’ contains word and their frequency of occurrence.

Given below is the implementation of feature generation.


def get_words_in_tweets(tweets):
    all_words = []
    for (words, sentiment) in tweets:
      all_words.extend(words)
    return all_words


def get_word_features(wordlist):
    wordlist = nltk.FreqDist(wordlist)
    word_features = wordlist.keys()
    return word_features

word_features = get_word_features(get_words_in_tweets(tweets))


Given below ‘extract_features’ function checks each input words whether it is present in the feature list or not.  

def extract_features(document):
    document_words = set(document)
    features = {}
    for word in word_features:
        features['contains(%s)' % word] = (word in document_words)
    return features




4.4.	Train the classifier:

We can apply the features to our classifier using the method apply_features. We pass the feature extractor along with the tweets list defined above.
training_set = nltk.classify.apply_features(extract_features, tweets)


Now that we have our training set ready in required format, we can train our classifier.

classifier = nltk.NaiveBayesClassifier.train(training_set)


The Naive Bayes classifier uses the prior probability of each label. It calculates frequency of each label in the training set. The word ‘awesome’ appears in 5 of 10 of the positive tweets but doesn’t appear in negative tweets. This means that the likelihood of the ‘positive’ label will be multiplied by 0.5 when this word is seen as part of the input.

4.5.	Most Informative Features     

print classifier.show_most_informative_features(10)     
    
     contains(best) = True           positi : negati =     13.7 : 1.0
     contains(bad) = True           negati : positi =     10.1 : 1.0
     contains(good) = True           positi : negati =      9.1 : 1.0
     contains(world) = True           positi : negati =      7.2 : 1.0
     contains(you) = True           negati : positi =      5.8 : 1.0
     contains(better) = True           positi : negati =      5.4 : 1.0
     contains(awesome) = True           positi : negati =      5.4 : 1.0
     contains(class) = True           positi : negati =      5.4 : 1.0
     contains(thought) = True           positi : negati =      5.0 : 1.0
     contains(striker) = True           positi : negati =      4.7 : 1.0


4.6.	Test classifier:

As we have already initialized the classifier, we can start testing the classifier to see whether we are getting right sentiment output for a new tweet. I had tested with ‘That game was awesome’ this tweet. Our classifier is able to detect that this tweet has a positive sentiment because of the word ‘awesome’.

tweet = 'I am very happy today.'
print classifier.classify(extract_features(tweet.split()))
positive

4.7.	Accuracy of the Classifier:

print(nltk.classify.accuracy(classifier, testing_set))


0.8

We have got high accuracy because training and test data were quite similar.

5.	Model Implementation using text blob classifier Python:

5.1.	 Load the training data and preprocess: Same preprocess steps are followed as mentioned in the 4.1.1. Section.

5.2.	Build the classifier:  Model is trained using Naïve Bayes classifier. Given below is the code for building classifier using given training data. Data is read from “csv” file and put into a list. ‘trainingData’ variable contains the list of tagged training data.[11]



from textblob.classifiers import NaiveBayesClassifier
from textblob import TextBlob

cl = NaiveBayesClassifier(trainingData)

5.3.	Test the classifier:

# Classify some text
print(cl.classify("She is a sweet person.."))  # "positive"
print(cl.classify("I don't like their pizza."))   #”negative”
print(cl.classify("She is wearing ugly dress."))#”negative”



5.4.	Most informative features


cl.show_informative_features(10)

       contains(best) = True           positi : negati =     12.4 : 1.0
       contains(bad) = True           negati : positi =      9.8 : 1.0
       contains(good) = True           positi : negati =      8.6 : 1.0
       contains(world) = True           positi : negati =      5.9 : 1.0
       contains(awesome) = True           positi : negati =      5.4 : 1.0
       contains(better) = True           positi : negati =      4.7 : 1.0
       contains(class) = True           positi : negati =      4.7 : 1.0
       contains(injury) = True           negati : positi =      4.6 : 1.0
       contains(you) = True           negati : positi =      4.6 : 1.0
       contains(thought) = True           positi : negati =      4.6 : 1.0




5.5.	Accuracy of classifier:

#Compute accuracy

print("Accuracy: {0}".format(cl.accuracy(test)))
0.83




If we give test data from training set without changing anything we can achieve 100% accuracy but given complete new data we got accuracy between 63 % to 83%. 



6.	Conclusion:

In lexicon approach, the main factor of getting better accuracy is to have accurate domain dictionary. But, in machine learning approach, the key factor for getting better accuracy is to have a large number of manually classified positive and negative tweets. In this study, an attempt has been made to classify sentiment for tweets using Naive Bayes (NB) classifier of two different python library. In future, we can extend the work by implementing new business rules. Also, we can try other classification methodology like support vector machine, maximum entropy classifier, stochastic gradient classifier, K nearest neighbor and others.





References

1.	Chetashri Bhadanea,Hardi Dalalb, Heenal Doshic, Sentiment analysis: Measuring opinions in  International Conference on Advanced Computing Technologies and Applications (ICACTA- 2015)
2.	Kennedy, A., Inkpen, D.Sentiment Classification of Movie and Product Reviews Using Contextual Valence Shifters. Computational Intelligence.2006. pp. 110-125.
3.	Turney, P. Thumbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised Classification of Reviews. Proceedings of ACL, Philadelphia, PA. July 2002. pp. 417-424.
4.	Kamps, J., Marx, M., Mokken, R. J.Using WordNet to Measure Semantic Orientation of Adjectives. LREC 2004. Volume IV, pp. 1115-1118.
5.	Andreevskaia, A., Bergler, S., Urseanu, M.All Blogs Are Not Made Equal: Exploring Genre Di_erences in Sentiment Tagging of Blogs.
International Conference on Weblogs and Social Media (ICWSM-     2007), Boulder, CO. 2007.
6.	Hemnaath, R., and Low, B. W. 2010. Sentiment Analysis Using Maximum Entropy and Support Vector Machine. Semantic Technology and Knowledge Engineering in 2010. Kuching,Sarawak.
7.	B. Pang, L. Lee and S. Vaithyanathan. Thumbs up?: sentiment classification using machine learning techniques. In EMNLP ’02: Proc. of the ACL-02 conf. on Empirical methods in natural language processing, pages 79–86. ACL, 2002.
8.	H. Yu and V. Hatzivassiloglou. Towards Answering Opinion Questions: Separating Facts from Opinions and Identifying the Polarity of Opinion Sentences. Proc. 2003 Conf. Empirical Methods in Natural Language Processing (Emnlp 03), ACM Press, 2003
9.	Twitter sentiment analysis using Python and NLTK ,laurent luce blog.
10.	Wikipedia
11.	 Building a Text Classification System (http://textblob.readthedocs.io/en/dev/classifiers.html )


Determining the polarity or semantic orientation of document:

Given any document, classify it as positive, negative or neutral is one of the key problem that you are going to solve using sentiment analysis. The problem has an official name, known as 
(Determining the polarity or semantics orientation of a document) There are many different ways of approaching this problem

1.	Rule based approach
2.	Machine learning based approach


1.	What is the current sentiment for a given search term?

Accept a search term from user and figure out what the current sentiment on twitter for the particular search term. Say you want to answer the question a general sentiment for donald Trumph in twitter is positive or negative. You could possibly answer today using this project.
So, how will we get to this ? We will start with answering what is sentiment Analysis and why it is becoming so interesting and important today in today’s context. Then we will move on to understand how this problem is solved. There are many different approaches: Rule based approaches and machine learning based approaches. We will understand the bit of each and concentrate obviously on the machine learning based approaches. We will also look at bunch of details on what kind of training data do we need, what are the different features that we could use, when you are solving this problem that brings us naturally to sentiment lexicons. Sentiment Lexicons are groups of words which have relationship, meanings and so on, defined in it. You could use this to figure out what are the features that you should use for sentiment Analysis. By this time, we are pretty close to having all the need to solve our problem. Before that we will take a segway and learn about Regular expression. Regular expressions are useful in many different context: from web scraping, text Processing. In our particular case, we want to learn about them because we want to process tweets or text we download from the internet. Once we have learnt about regular expressions we will be ready to start coding up. There is a code along and  you can code along with me. That is to
1.	Download stuff from twitter and access the API
2.	Use Regular expression to preprocess your tweet.
3.	Use NLTK and sci-kit learn to classify your tweet.
4.	Print out what is the what is prevailing sentiment for a particular search term.



Sentiment Analysis- What’s All the fuss About?
People are spending more and more time online these days. They are buying products from place like Amazon, they read news and reviews or blogs, they watch videos, movies; a bunch of things online. And then they express their opinion about these things. They do this everywhere whether it’s a YouTube videos, comment section, twitter, reviews page of imdb. They keep expressing their opinion about the thing whether they like it or not and other people read these opinions. So, anyone who is selling product or providing a service, whether it’s offline or online. You could be a marketplace like Flipkart or restaurant in Bangalore whoever you are, you want and need to understand what people are saying about your product or service, do they like your product or service  and what they  tell other people about your product or service to try or not. Why it is so important. 
I found this quote online other day and I found it’s perfectly explains

“A brand is no longer what we tell the customer it is. It is what customers tell each other it is. ” Scott Cook.

Whatever people feel about your brand , whether they feel it’s great  or horrible or they would recommend other people to try or not. This completely depends on whether your customer feel about your product. They tell other people about their experience through reviews, comments, tweets, status message. All of these  carry information about people’s opinion. Do they like your brand or  hate it or trust it or feel anger about it for not being worthy. Lots of companies hire mountains of people these days to monitor social media to find opinions of people about their product or services.  With the explosion of data, obviously it is very difficult to keep a track of everything. So, you would like to have your machine to do it for you. Machine should be parsing these reviews, comments, emails and so on . Machine would tell you information about the opinion or feeling of this pieces of text or documents are expressing. This is exactly the idea of opinion mining.

Sentiment Analysis is called as subjectivity analysis. Basically it is a field of natural language processing and objective is to try and extract subjective information like opinions and feelings from these pieces of text. So, here are few examples of the problems:

1.	Are people satisfied or dissatisfied with my service?
2.	Is this review positive or negative?
3.	What is the intensity of positivity or negativity of the reviews?
4.	Support a new smartphone is launched and you want to know whether customer liked the camera, screen or other?
5.	How are people responding to an AD or an event?
6.	How are people reacting to a candidate’s speech during an election campaign?

For factual statements like “This is a car” , there is no subjectivity involved so you can classify this as neutral.

Rule based approaches require you to study vocabulary and a lot of lexical patterns, features and so on and come up with rules. You will start with as large corpus of data and go through it and mark each text or document as positive or negative based on human judgment, then you will have large set of rules based on what are the words presents in that particular text, what are the relationships between the words in that text and so on. Now these rules will determine what the polarity of a particular document.

Machine learning based approach is based on statistics and machine learning techniques which learn from the data themselves. You don’t have to give all the rules to the particular algorithm as long as you provide some training data and tell the algorithm to what to look for. It will figure out whether polarity of the text is positive or negative.

Let’s take some real example for each of this :
Here is one example of a rule based approach:

1.	Look at all the words in the text and classify each of them as positive/ negative.
2.	For example, the word “like” is in positive sense or word “hate” is in negative sense. You would look at all the words in the text and see how many positive words and how many negative words are present in the text. If there are more positive words than negative words. Then classify the document as positive and if it is other way around classify it as negative.
3.	Counting the number of the positive or negative words is the simplest rule based approach that you can think of.
4.	All you need here is some way to classify each word as positive or negative. How do you say that some words are positive and some words are negative? For this you would need something called as lexicon. A lexicon is something which contains meanings of the words. A Lexicon is a resource where all words which we would ever encounter have been classified as positive or negative. So, you would need a list of positive words and a list of negative words. Now given all the thousands of word in English language or any other language that you may think of. Doing this by yourselves as in setting up lexicons is humongous task. So, there are hand annotated lexicons which have been made available by university researchers which made life easier for you and me who are trying to do sentiment analysis. All we need to do is to get the lexicons and use then for classification of words as positive or negative. It is as simple as look up. For example: “I really like the new IPHONE. It’s awesome.”  It got clearly two positive words i.e., “like” and “awesome” and other words are kind of neutral. Then we have the text : “I hate Apple!” .This sentence clearly has “hate” as negative word. By using rule based approach we would clearly be able to classify these two texts as “Positive” or “negative text. The problem is that people don’t express their opinion so simply, they use very complex mechanism like sarcasm, irony and different sorts of lexical relationships between the words to express what they are feeling. There is a famous example of a letter someone wrote saying:
“I went to your competitor store. I loved it. It was so wonderful bla bla bla bla..You suck”.
If you count the positive words in that that particular letter. It could be huge. you could see more positive words than negative words but unfortunately it is about your competitor. People express various intensity of emotion while expressing the opinions. For example, when someone is telling that “I really like the new IPHONE. It’s awesome.” Really is an intensifier and also used the word “awesome” which is also having much higher intensifier of positivity.
For any good sentiment analysis tool, it should come up with ruleset which takes of all of these patterns of sentences.

There have been many rule based approaches suggested for sentiment analysis some are very complex and very good at classifying as well. They actually take care of all these different kinds of relationship and in fact use the most important kind of information that these relationships provide. One example of a famous rule based sentiment analysis approach is vader which implemented as module in nltk.  It performs really well even against some machine learning approaches. It takes into account many many different rules. In fact the researchers, who build vader spent a long time studying a lot of corpora and they have human experts to evaluate this corpora to come up with gold standard rules. This is the name they gave for top five categories of rules which help anybody identify whether a document is positive or negative. Here is the example of the rules they came up with “this food is AMAZING!!!:)”   
When people express their opinion they use of CAPS and exclamation points, emoticons if they really like something or really hate something. So, taking these kind of punctuation and Capitalization into the consideration gives us a lot of information about the sentiment expressed.

Words that signal a shift in emotion- BUT, HOWEVER.
“I liked the book initially but not after the first 100 pages” 
Vader has taken into account this kind of shifts in emotion. People usually use intensifier or modifier to describe how they are feeling about something. These are words like extremely, hardly, really, very. For example, “This restaurant is really good.”  This means it is just not good or “this food is hardly edible” means food is to say that it’s not edible at all.

But in general, based on the context of what the text you are looking at it, may be people write differently on twitter when compared to email because in twitter we write in short line of text. In this kind s of context, vader may not always work/ rule based approach may not work. This is where machine learning comes into play.


Machine learning based approach:

Deciding in which machine learning approach to use is pretty straight forward. Just approach it a classification problem. As a problem we have encountered this kind of thing many times before. Classify a tweet or review as positive or negative.
There are many classification algorithms but Naive bayes and support vector machine are most commonly used techniques for sentiment analysis. We could use any different method  as well apart from those mentioned algorithms like knn, logistic regression, artificial neural network etc. Every machine learning algorithm need some tricky detail:
1.	What do you use as training  data?
2.	What features do you choose? 
Every machine learning problem requires you to make you a bunch of choices.  Classification is a supervised learning problem. It takes labelled data and learns from it and then applies these learnings to new instances. So, you would give the classification algorithm the training data already labelled as positive or negative. Based on its insight from that, when you give it a new instance it will classify as positive or negative. Now how the classifier learns will depend on the features you provide. If you represent the document as just the words in the document, or if you represent it as pairs of words from the document or if you represent it as each word and its frequency i.e., numbers of time it occurred in the document or represent it as top N important words in the document which ever feature you choose to represent a document would determine how the classifier perform. Any classifier requires this two inputs: training data and represent the training data as feature vector of different attributes of the document.
Let’s look at what we can use as training as training data. You need a training data which are already marked as positive or negative. 

The simplest way is to look at the individual words in the document ; this is also known as bag of words. Bag words means you take all the distinct words that occur in a particular document and represent those documents using those words; 
For using Naive Bayes classifier you need to compute posterior probabilities. It is the probability that a document is positive or negative given some evidence. Here the evidence is the words present in the document. So, we will calculate the conditional probability that the document is positive given the words in the documents. You can compute this using the Bayes rules.
This probability is basically product of the prior probability that the document is positive. This is simply the proportion of number of positive document we have out of the entire corpus. 
Number of documents positive/ total number of documents in the corpus. This is the prior probability that a document is positive. We will multiply this with the conditional probability of the words given that the document is positive and notice how we are multiplying the probability of each of the word.





What is machine learning?

Machine learning is a field of study that gives computers ability to learn without being explicitly programmed.

What does tf-idf mean? 						 						 				 				 					 Tf-idf stands for term frequency-inverse document frequency, and the tf-idf weight is a weight often used in information retrieval and text mining. This weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus. The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus. Variations of the tf-idf weighting scheme are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query.
One of the simplest ranking functions is computed by summing the tf-idf for each query term; many more sophisticated ranking functions are variants of this simple model.
Tf-idf can be successfully used for stop-words filtering in various subject fields including text summarization and classification.
How to Compute  Tf-idf :
Typically, the tf-idf weight is composed by two terms: the first computes the normalized Term Frequency (TF), aka. the number of times a word appears in a document, divided by the total number of words in that document; the second term is the Inverse Document Frequency (IDF), computed as the logarithm of the number of the documents in the corpus divided by the number of documents where the specific term appears.
TF: Term Frequency, which measures how frequently a term occurs in a document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length (aka. the total number of terms in the document) as a way of normalization:
TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document).
IDF: Inverse Document Frequency, which measures how important a term is. While computing TF, all terms are considered equally important. However it is known that certain terms, such as "is", "of", and "that", may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones, by computing the following:
IDF(t) = log_e(Total number of documents / Number of documents with term t in it). 	
We now combine the definitions of term frequency and inverse document frequency, to produce a composite weight for each term in each document. The tf-idf weighting scheme assigns to term   a weight in document   given by

	
In other words,   assigns to term   a weight in document   that is
1.	highest when   occurs many times within a small number of documents (thus lending high discriminating power to those documents);
2.	lower when the term occurs fewer times in a document, or occurs in many documents (thus offering a less pronounced relevance signal);
3.	lowest when the term occurs in virtually all documents.
See below for a simple example.
							
Example:
Consider a document containing 100 words wherein the word cat appears 3 times. The term frequency (i.e., tf) for cat is then (3 / 100) = 0.03. Now, assume we have 10 million documents and the word cat appears in one thousand of these. Then, the inverse document frequency (i.e., idf) is calculated as log(10,000,000 / 1,000) = 4. Thus, the Tf-idf weight is the product of these quantities: 0.03 * 4 = 0.12




Data Mining: Classification VS Clustering (cluster analysis)

CLASSIFICATION	CLUSTERING
We have a Training set containing data that have been previously categorized.
Based on this training set, the algorithms finds the category that the new data points belong to	We do not know the characteristics of similarity of data in advance.
Using statistical concepts, we split the datasets into sub-datasets such that the Sub-datasets have “Similar” data
Since a Training set exists, we describe this technique as Supervised learning	Since Training set is not used, we describe this technique as Unsupervised learning
Example:We use training dataset which categorized customers that have churned. Now based on this training set, we can classify whether a customer will churn or not.	Example:We use a dataset of customers and split them into sub-datasets of customers with “similar” characteristics. Now this information can be used to market a product to a specific segment of customers that has been identified by clustering algorithm


How to calculate Levenshtein distance?

Edit distance, also known as Levenshtein distance or evolutionary distance is a concept from information retrieval and it describes the number of edits (insertions,deletions and substitutions) that have to be made in order to change one string to another. It is the most common measure to expose the dissimilarity between two strings.

“Edit distance” between two strings: the minimum number of single-character insertions, deletions, or replacements required to convert one string into another. 


How to calculate the Euclidean and Manhattan distances by hand using the following values:

x1:  1.0, 3.2, 4.8, 0.1, 3.2, 0.6, 2.2, 1.1
x2:  0.1, 5.2, 1.9, 4.2, 1.9, 0.1, 0.1, 6.0


Manhattan distances:
For example, if x=(a,b) and y=(c,d), the Manhatten distance between x and y is |a−c|+|b−d|.

●	|1−0.1|+|3.2−5.2|+|4.8−1.9|+|0.1−4.2|+|3.2−1.9|+|0.6−0.1|+|2.2−0.1|+|1.1−6.0|
●	compute the absolute values
0.9+2+2.9+4.1+1.3+.5+2.1+4.9
●	sum them up to get 18.7


Euclidean distances: 
Take the square root of the sum of the squares of the differences of the coordinates.
For example, if x=(a,b) and y=(c,d), the Euclidean distance between x and y is root(sq(a−c)+sq(b−d))
 


Hamming distance: The Hamming distance between two strings of equal length is the number of positions at which the corresponding symbols are different.
The Hamming distance between:
●	"karolin" and "kathrin" is 3.
How to Calculate Hamming Distance?

Step 1
		 			
Ensure the two strings are of equal length. The Hamming distance can only be calculated between two strings of equal length. String 1: "1001 0010 1101" String 2: "1010 0010 0010"
		
Step 2
		 			
Compare the first two bits in each string. If they are the same, record a "0" for that bit. If they are different, record a "1" for that bit. In this case, the first bit of both strings is "1," so record a "0" for the first bit.
		
Step 3
		 			
Compare each bit in succession and record either "1" or "0" as appropriate. String 1: "1001 0010 1101" String 2: "1010 0010 0010" Record: "0011 0000 1111"
		
Step 4
		 			
Add all the ones and zeros in the record together to obtain the Hamming distance. Hamming distance = 0+0+1+1+0+0+0+0+1+1+1+1 = 6



N-gram /q-gram distance:

	You have certainly seen the fuzzy text searches everywhere. For example you type "stck" but you actually mean "stack"! Ever wondered how does this stuff work?
There are plenty of algorithms to do fuzzy text matching, each with its own pro and cons. The most famous ones are edit distance and qgram. I want to focus on qgrams today and implement a sample.
Basically qgrams are the most suitable fuzzy string matching algorithm. It is pretty simple. "q" in qgram will be replaced with a number like 2-gram or 3-gram or even 4-gram.
2-gram means that every word is broken into a set of two character grams.

Example:
Suppose ‘Stack’,’database’ and ‘stat’ these words exist in the database for matching nearest string with the word ‘stck’

 "Stack" will be broken into a set of {"st", "ta", "ac", "ck"} or "database" will be broken into {"da","at","ta","ba","as","se"} and “stat” will be broken into {“st”,”ta”,”at”}.
Once the words are broken into 2-grams, we can search the database for a set of values instead of one string. For example if user mistyped "stck", any search for "stck" will not match "stack" because "a" is missing, but the 2-gram set {"st","tc","ck"} has 2 rows in common with the 2-gram set of stack! Bingo we found a pretty close match. It has nothing in common with the 2-gram set of database and only 1 in common with the 2-gram set of "stat" so we can easily suggest the user that he meant to type: first "stack" or second, "stat".
Jaccard similarity coefficient (originally coined coefficient de communauté by Paul Jaccard):
It is a statistic used for comparing the similarity and diversity of sample sets. The Jaccard coefficient measures similarity between finite sample sets, and is defined as the size of the intersection divided by the size of the union of the sample sets:

Jaccard    similarity      = coefficient		 

where
p = number of variables that are positive for both
q= number of variables that are positive in Q but not D
d= number of variables that are positive in D but not Q

If p1 = 10111 and p2 = 10011
The total number of each combination attributes for p1 and p2:
●	M11 = total number of attributes(Union) where p1 & p2 have a value 1,
●	M01 = total number of attributes where p1 has a value 0 & p2 has a value 1,
●	M10 = total number of attributes where p1 has a value 1 & p2 has a value 0,
●	M00 = total number of attributes where p1 & p2 have a value 0.

Jaccard similarity coefficient = J = intersection/union = M11/(M01 + M10 + M11) = 3 / (0 + 1 + 3) = 3/4,
Jaccard distance = J' = 1 - J = 1 - 3/4 = 1/4, Or J' = 1 - (M11/(M01 + M10 + M11)) = (M01 + M10)/(M01 + M10 + M11) = (0 + 1)/(0 + 1 + 3) = ¼




Why is Euclidean distance not a good metric in high dimensions?
It depends on your data. If you have a lot of useless attributes, Euclidean distance will become useless. If you could easily embed your data in a low-dimensional data space, then Euclidean distance should also work in the full dimensional space. In particular for sparse data, such as TF vectors from text, this does appear to be the case that the data is of much lower dimensionality than the vector space model suggests.




K-means Algorithm example:

1.	Organize the data.
2.	Group the data into clusters. Each cluster should consist of the points of data closest to it.
3.	For each cluster, add the values of all members. For example, if a cluster of data consisted of the points (80, 56), (75, 53), (60, 50), and (68,54), the sum of the values would be (283, 213).
4.	Divide the total by the number of members of the cluster. In the example above, 283 divided by four is 70.75, and 213 divided by four is 53.25, so the centroid of the cluster is (70.75, 53.25).
5.	Plot the cluster centroids and determine whether any points are closer to a centroid of another cluster than they are to the centroid of their own cluster. If any points are closer to a different centroid, redistribute them to the cluster containing the closer centroid.
6.	Repeat Steps 3, 4 and 5 until all points of data are in the cluster containing the centroid to which they are closest.

http://people.revoledu.com/kardi/tutorial/kMean/NumericalExample.htm


What is overfitting?
What is dimension reduction?
What is lemmatization?


		
	
What is Synthetic Data?

	Synthetic data are "any production data applicable to a given situation that are not obtained by direct measurement". Synthetic data are generated to meet specific needs or certain conditions that may not be found in the original, real data. 



SpellChecker:
http://www.params.me/2012/07/spelling-corrector-algorithm-in-java.html
http://bakedcircuits.wordpress.com/2013/08/10/simple-spell-checker-in-java/
https://wiki.apache.org/lucene-java/SpellChecker




Precision & Recall:
https://www.creighton.edu/fileadmin/user/HSL/docs/ref/Searching_-_Recall_Precision.pdf

Reference:
http://www.thecloudavenue.com/p/machinelearning.html
http://www.datasciencecentral.com/profiles/blogs/66-job-interview-questions-for-data-scientists
http://career.guru99.com/top-50-interview-questions-on-machine-learning/
http://staff.utia.cas.cz/vomlel/slides/presentace-karny.pdf
http://en.wikipedia.org/wiki/Support_vector_machine















------------------------------


2.	Levels of Sentiment Analysis
There are multiple studies that define sentiment analysis with different levels of the analyzed texts. It includes word or phrase [1-5-6], sentence [7-8], document level [9-10-4], and user level [11-12]. Word level sentiment analysis explore the orientation of the words or phrases in the text and their effect on the overall sentiment, while sentence level considers sentences which express a single opinion and try to define its orientation[1-2]. The document level opinion mining is looking at the overall sentiment of the whole document, and user level sentiment searches for the possibility that connected users on the social network could have the same opinion [12]. 

3.	Sentiment Analysis approaches:
There exist three approaches towards sentiment analysis: lexicon based methods, machine learning based methods and linguistic analysis [1-2]. 
3.1.	 Lexicon-based algorithms: 
A lexicon based method depends on a predefined list or corpus of words with a certain polarity. An algorithm is then searching for those words, counting them or estimating their weight and measuring the overall polarity of the text [11-13].

3.2.	 Machine Learning based algorithms: 
This technique creates a model by training the classifier with a data set acquired from the considering types of documents. This means that one must first gather a dataset (with examples for positive and negative – for supervised learning), extract the features/words from the examples and then train the algorithm based on the examples. The next step is validation on another dataset to estimate accuracy and decide whether it is able to detect the right features and give the right classification.

3.3.	Linguistic analysis:
Lastly the linguistic approach uses the syntactic characteristics of the words or phrases, the negation, and the structure of the text to determine the text orientation. This approach is usually combined with a lexicon based method [1-2-8].
Choosing which method you will use heavily depends on the application, domain and language. Given below is the detail description on machine learning approach.

4.	Machine Learning Methodology
The sentiment classification methods using ML approach can be divided into supervised and unsupervised learning methods. The supervised methods make use of a large number of labeled training documents. The unsupervised methods are used when it is difficult to find these labeled training documents. As we are interested to classify data into predefined sentiment labels, so supervised machine learning is mostly used for this purpose. Given below are the stages followed during sentiment analysis using machine learning based approach.

4.1.	Preprocess:  
It is the stage of cleansing a text by removing some uninformative parts from the text. Pre-processing plays an important role to improve the performance of the classifier and speed up the classification process. Thus, for example, many words, which exist in the text, do not have any impact to text classification or have even a negative impact and, so, should be removed during this stage. The original text can contain lots of noise and un-necessary data such as HTML tags, scripts, hyperlinks, underlying images, advertisements etc. Classification becomes very difficult with uninformative data as it increases the dimensionality of the problem since each word in the text is considered as one dimension. The whole pre-processing involves several steps: online text cleansing, white space removal, expanding abbreviation, stemming, stop words removal, negation handling, etc.
After pre-processing and before model training and validation, there are two important stages: Features generation and selection. 
4.2.	Feature generation:  
Features are the attributes of data which are the most useful for capturing patterns in the data. Features in the context of opinion mining are the words, terms or phrases that strongly express the opinion as positive or negative. This means that they have a higher impact on the orientation of the text than other words in the same text [14].
4.3.	Feature Selection: 
There are several methods that are used in feature selection, where some are syntactic, based on the syntactic position of the word such as adjectives, and some are univariate, based on each feature's relation to a specific category such as chi squared and information gain, and some are multivariate using genetic algorithms and decision trees based on features subsets [4].
There are several ways to assess the importance of each feature by attaching a certain weight in the text. The most popular ones are: Feature Frequency (FF), Term Frequency Inverse Document Frequency (TF-IDF), and Feature Presence (FP). FF is the number of occurrences in the document. TF-IDF is given by				
				TF-IDF =FF * Log(N/DF)
where N indicates the number of documents, and DF is the number of documents that contains this feature [15]. FP takes the value 0 or 1 based on the feature absent or presence in the document.


4.4.	Training and Testing Model: 
Once the features were generated and the most relevant were selected, each of the tweets from the training dataset expressed in terms of the attributes. During the training process the presence of each attribute is checked for each of the classes (positive and negative) and then tested on another set whether it is able to detect the right features and give the right classification.
5.	Conclusion
Sentiment Analysis is a challenging job. It is not only difficult from an algorithmic perspective but also from human perspective. People may have different opinions on the sentiment of a piece of text, especially on a tweet, because of the subjective nature of sentiment analysis.
Another major challenge related to sentiment analysis is that the sentiments of expressions change depending on domain and context. For example, certain adjectives may mean a positive sentiment about a particular aspect of a hotel (for example 'cheap price’), but the same adjective derive negative sentiment when someone says 'cheap quality' about certain product.
Because of the huge volume of real time social media data, it is difficult for human being to conclude on the people opinion about a certain product. For overcoming the challenges, people felt the need of automatically classifying the sentiment.  Using lexicon-based techniques with large dictionaries enables us to achieve reasonably good results. However, lexicon is not available in some languages. On the other hand Machine Learning based techniques deliver good results nevertheless, they require obtaining datasets and training
The most popular classification algorithms are the Naive Bayes, Support Vector Machines (SVMs) and maximum entropy. Classification accuracy of these algorithms depends on the list of features. Barbosa et al. reports better results for SVMs while Pak et al. obtained better results for Naive Bayes. Naive Bayes performs the best according to many researches [16-17]. Machine learning classifiers such as naive Bayes, maximum entropy and support vector machine (SVM) are used in [3] for sentiment classification to achieve accuracies that range from 75% to 83%.
References:
[1] Emma Haddia, Xiaohui Liua, Yong Shib,The Role of Text Pre-processing in Sentiment Analysis in Information Technology and Quantitative Management (ITQM2013). 
 [2] M. Thelwall, K. Buckley, G. Paltoglou, Sentiment in twitter events, Journal of the American Society for Information Science and Technology 62 (2)(2011) 406 418.
[3] B. Pang, L. Lee, S. Vaithyanathan, Thumbs up? sentiment classification using machine learning techniques, in: Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2002.
[4] A. Abbasi, S. France, Z. Zhang, H. Chen, Selecting attributes for sentiment classification using feature relation networks, Knowledge and Data Engineering, IEEE Transactions on 23 (3) (2011) 447 462.
[5] P. Tetlock, M. Saar- rnal of Finance 63 (3) (2008) 1437 1467.
[6] T. Wilson, J. Wiebe, P. Hoffmann, Recognizing contextual polarity in phrase-level sentiment analysis, in: Proceedings of the Human Language Technology Conference and the Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP), 2005, pp. 347 354.
[7] H. Yu, V. Hatzivassiloglou, Towards answering opinion questions: separating facts from opinions and identifying the polarity of opinion sentences, in: Proceedings of the conference on Empirical methods in natural language processing, EMNLP-2003, 2003, pp. 129 136.
[8] L. Tan, J. Na, Y. Theng, K. Chang, Sentence-level sentiment polarity classification using a linguistic approach, Digital Libraries: For Cultural Heritage, Knowledge Dissemination, and Future Creation (2011) 77 87.
[9] S. R. Das, News Analytics: Framework, Techniques and Metrics, Wiley Finance, 2010, Ch. 2, the Handbook of News Analytics in Finance.
[10] B. Pang, L. Lee, S. Vaithyanathan, Thumbs up? sentiment classification using machine learning, Association for Computational Linguistics, 2002, pp. 97 86, conference on Empirical Methods in Natural Language processing EMNLP.
[11] P. Melville, W. Gryc, R. Lawrence, Sentiment analysis of blogs by combining lexical knowledge with text classification, in: Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining, ACM, 2009, pp. 1275 1284.
[12] C. Tan, L. Lee, J. Tang, L. Jiang, M. Zhou, P. Li, User-level sentiment analysis incorporating social networks, Arxiv preprint arXiv:1109.6018.
[13] X. Ding, B. Liu, P. Yu, A holistic lexicon-based approach to opinion mining, in: Proceedings of the international conference on Web search and web data mining, ACM, 2008, pp. 231 240
[14] I. Feinerer, K. Hornik, D. Meyer, Text mining infrastructure in r, Journal of Statistical Software 25 (5) (2008) 1 54.
[15] J.-C. Na, H. Sui, C. Khoo, S. Chan, Y. Zhou, Effectiveness of simple linguistic processing in automatic sentiment classification of product reviews, in:
Conference of the International Society for Knowledge Organization (ISKO), 2004, pp. 49 54.
[16]. A. Pak and P. Paroubek, \Twitter as a corpus for sentiment analysis and opinion
mining," in Proceedings of the Seventh International Conference on Language Re-
sources and Evaluation (LREC'10) (N. C. C. Chair), K. Choukri, B. Maegaard,
J. Mariani, J. Odijk, S. Piperidis, M. Rosner, and D. Tapias, eds.), (Valletta,
Malta), European Language Resources Association (ELRA), may 2010.
[17]. L. Barbosa and J. Feng, \Robust sentiment detection on twitter from biased and
noisy data," in Proceedings of the 23rd International Conference on Computational
Linguistics: Posters, COLING '10, (Stroudsburg, PA, USA), pp. 36{44, Association
for Computational Linguistics, 2010.
