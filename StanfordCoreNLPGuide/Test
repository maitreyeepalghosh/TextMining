
BOT_NAME = 'newsHack'

SPIDER_MODULES = ['newsHack.spiders']
NEWSPIDER_MODULE = 'newsHack.spiders'



ITEM_PIPELINES = [
    'scrapyelasticsearch.scrapyelasticsearch.ElasticSearchPipeline',
]

ELASTICSEARCH_SERVERS = ['server ip address']
ELASTICSEARCH_INDEX = 'hack_news_data'
ELASTICSEARCH_TYPE = 'gsites'
ELASTICSEARCH_UNIQ_KEY = 'link' # Custom uniqe key
-------------------------------------------------------------
items.py

from scrapy.item import Item, Field

class HackNewsItem(Item):
    # define the fields for your item here like:
    # name = scrapy.Field()i
    title = Field()
    link = Field()
    content= Field()

-----------------------------------------------

pipelines.py

class NewshackcrawlerPipeline(object):
    def process_item(self, item, spider):
        return item
        
------------------------------

hack_news.py

from scrapy.spider import BaseSpider, Rule
from scrapy.selector import Selector
import scrapy
from FilterContent import cleanUp
from scrapy.linkextractors import LinkExtractor
from newsHackCrawler.items import HackNewsItem



class DmozSpider(BaseSpider):
   name = "newshack"
   allowed_domains = ["indianexpress.com"]
   start_urls = [
       "http://indianexpress.com/article/business/banking-and-finance/urjit-patel-rbi-governor-500-1000-notes4364689/"
   ]
   rules = (
         Rule(LinkExtractor(allow=('\.html', ))),
    )

   def parse(self, response):
       sel = Selector(response)      
       title = sel.xpath('//title/text()')
       item = HackNewsItem()       
       item['content'] = cleanUp().grabContent("",(sel.xpath('//body').extract())[0])       
       item['title'] = title.extract()
       item['link'] =response.url
       yield item
       for url in sel.xpath('//a/@href').extract():
            url1 = response.urljoin(url)            
            yield scrapy.Request(url1, callback=self.parse)
            
            
  ----------------------------------------------------------------------
  
  #!/usr/bin/env python
# encoding=utf8
import urllib 
import re
import json
import urllib, urllib2
import unicodedata
import sys, getopt
import os
import urlparse
import HTMLParser
from bs4 import BeautifulSoup
#from BeautifulSoup import BeautifulSoup
import codecs
#import nltk
import requests, base64
import sys
reload(sys)
sys.setdefaultencoding('utf8')
class cleanUp:
	NEGATIVE    = re.compile("comment|meta|footer|footnote|foot")
	POSITIVE    = re.compile("post|hentry|entry|content|text|body|article|table")
	PUNCTUATION = re.compile("""[!"#$%&\'()*+,-./:;<=>?@[\\]^_`{|}~]""")


        def remove_non_ascii(self, text):
              return ''.join([i if ord(i) < 128 else ' ' for i in text])

	def grabContent(self,link, html):
		#print "########## inside grabContent #########"
		replaceBrs = re.compile("<br */? *>[ \r\n]*<br */? *>")
		html = re.sub(replaceBrs, "</p><p>", html)
		try:
			soup = BeautifulSoup(html)
		except HTMLParser.HTMLParseError:
			return ""
    
    # REMOVE SCRIPTS
		for s in soup.findAll("script"):
			s.extract()
	
		#allParagraphs = soup.findAll("p")
		allParagraphs = soup.findAll(['p','table'])
                #print type(allParagraphs)
                #append.allParagraphs(soup.findAll('table'))
		topParent     = None
                #table = soup.findAll("table")
                #print table
	
                #print allParagraphs
		parents = []
		for paragraph in allParagraphs:
			parent = paragraph.parent
                        #print dir(parent)
			if (parent not in parents):
				parents.append(parent)
				parent.score = 0
	
				if (parent.has_attr("class")):
                                    if type(parent["class"]).__name__=='list':
                                        parent_class = ''.join(map(str,parent["class"]))
					if (self.NEGATIVE.match(parent_class)):
						parent.score -= 50
					if (self.POSITIVE.match(parent_class)):
						parent.score += 25
                                    else:  
					if (self.NEGATIVE.match(parent["class"])):
						parent.score -= 50
					if (self.POSITIVE.match(parent["class"])):
						parent.score += 25
						
				if (parent.has_attr("id")):
					if (self.NEGATIVE.match(parent["id"])):
						parent.score -= 50
					if (self.POSITIVE.match(parent["id"])):
						parent.score += 25
                 
	
			if (parent.score == None):
				parent.score = 0
			
			innerText = paragraph.renderContents() #"".join(paragraph.findAll(text=True))
			if (len(innerText) > 10):
				parent.score += 1
				
			parent.score += innerText.count(",")
			
		for parent in parents:
			if ((not topParent) or (parent.score > topParent.score)):
				topParent = parent
	
		if (not topParent):
			return ""
				
		# REMOVE LINK'D STYLES
		styleLinks = soup.findAll("link", attrs={"type" : "text/css"})
		for s in styleLinks:
			s.extract()
	
		# REMOVE ON PAGE STYLES
		for s in soup.findAll("style"):
			s.extract()
	
		# CLEAN STYLES FROM ELEMENTS IN TOP PARENT
		for ele in topParent.findAll(True):
			del(ele['style'])
			del(ele['class'])
			
		self.killDivs(topParent)
		self.clean(topParent, "form")
		self.clean(topParent, "object")
		self.clean(topParent, "iframe")
		
		#fixLinks(topParent, link)
		#print topParent.renderContents()
		#print "########## exiting grabContent #########"
		#return topParent.renderContents()
                return  self.remove_non_ascii(BeautifulSoup(topParent.renderContents()).get_text().replace("\t", "").replace("\r", "").replace("\n", "")).decode('unicode_escape').encode('ascii','ignore')
                #return BeautifulSoup(topParent.renderContents()).get_text()
		
	
	
	def clean(self,top, tag, minWords=1000):
		tags = top.findAll(tag)
	
		for t in tags:
			if (t.renderContents().count(" ") < minWords):
				t.extract()


	def killDivs(self,parent):
		
		divs = parent.findAll("div")
		for d in divs:
			p     = len(d.findAll("p"))
			img   = len(d.findAll("img"))
			li    = len(d.findAll("li"))
			a     = len(d.findAll("a"))
			embed = len(d.findAll("embed"))
			pre   = len(d.findAll("pre"))
			code  = len(d.findAll("code"))
		
			if (d.renderContents().count(",") < 10):
				if ((pre == 0) and (code == 0)):
					if ((img > p ) or (li > p) or (a > p) or (p == 0) or (embed > 0)):
						d.extract()

                    
	def cleanData(self,url):
		#url  = "http://advanceindiana.blogspot.com/2014/01/did-marions-mayor-wayne-seybold-gamble.html"
		print "########## inside clean Data ##########"
                try:
                     response = urllib2.urlopen(url, timeout=150)
                except urllib2.URLError:
                     print "Bad URL or timeout"
		#response  = urllib2.urlopen(url) 
                print 'got url response'
		html = response.read()
		html.decode('utf-8','ignore')
		html1 = "".join(i for i in html if ord(i)<128)
                #print html1
		contents_details = self.grabContent("", html1)
		print "########## exiting cleanData #########k"
                #return BeautifulSoup(contents_details).get_text().replace("\t", "").replace("\r", "").replace("\n", "")
                #return (BeautifulSoup(contents_details).get_text()).encode('ascii', 'ignore').decode('ascii')
                return self.remove_non_ascii(BeautifulSoup(contents_details).get_text()).decode('unicode_escape').encode('ascii','ignore')
		#return nltk.clean_html(contents_details)

if __name__ == "__main__":
                        cleanup = cleanUp()
		        url  = "http://www.biography.com/people/narendra-modi"
                        url = "https://www.online.citibank.co.in/customerservice/cs-faq.htm"
                                       raw = cleanup.cleanData(url)
                        print raw
                        #print raw.rstrip('\r\n').replace("\n", " ").replace("\t", " ")
                        #print ''.join(raw.splitlines())
                        #print raw.rstrip('\r\n')

